{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API \n",
    "\n",
    "## API v1/messages\n",
    "\n",
    "```bash\n",
    "curl https://api.anthropic.com/v1/messages \\\n",
    "     --header \"x-api-key: $ANTHROPIC_API_KEY\" \\\n",
    "     --header \"anthropic-version: 2023-06-01\" \\\n",
    "     --header \"content-type: application/json\" \\\n",
    "     --data \\\n",
    "'{\n",
    "    \"model\": \"claude-3-7-sonnet-20250219\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Hello, world\"}\n",
    "    ]\n",
    "}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "my_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "\n",
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic(\n",
    "    api_key=my_api_key,\n",
    "    base_url = \"http://localhost:11434\", \n",
    ")\n",
    "\n",
    "our_first_message = client.messages.create(\n",
    "    model=\"llama3.2\",\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hi there! Please write me a haiku about a pet chicken\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(our_first_message.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## messages format\n",
    "\n",
    "The messages parameter expects a list of message dictionaries, where each dictionary represents a single message in the conversation. Each message dictionary should have the following keys:\n",
    "\n",
    "role: A string indicating the role of the message sender. It can be either \"user\" (for messages sent by the user) or \"assistant\" (for messages sent by Claude).\n",
    "content: A string or list of content dictionaries representing the actual content of the message. If a string is provided, it will be treated as a single text content block. If a list of content dictionaries is provided, each dictionary should have a \"type\" (e.g., \"text\" or \"image\") and the corresponding content. For now, we'll leave content as a single string.\n",
    "\n",
    "\n",
    "![](./imdges/message_format.png)\n",
    "\n",
    "\n",
    "The messages format allows us to structure our API calls to Claude in the form of a conversation, allowing for context preservation: The messages format allows for maintaining an entire conversation history, including both user and assistant messages. This ensures that Claude has access to the full context of the conversation when generating responses, leading to more coherent and relevant outputs.\n",
    "\n",
    "\n",
    "## message response\n",
    "\n",
    "The most important piece of information is the content property: this contains the actual content the model generated for us. This is a list of content blocks, each of which has a type that determines its shape.\n",
    "\n",
    "\n",
    "![](./imdges/response_message_format.png)\n",
    "\n",
    "\n",
    "In addition to content, the Message object contains some other pieces of information:\n",
    "\n",
    "- id - a unique object identifier\n",
    "- type - The object type, which will always be \"message\"\n",
    "- role - The conversational role of the generated message. This will always be \"assistant\".\n",
    "- model - The model that handled the request and generated the response\n",
    "- stop_reason - The reason the model stopped generating. We'll learn more about this later.\n",
    "- stop_sequence - We'll learn more about this shortly.\n",
    "- usage - information on billing and rate-limit usage. Contains information on:\n",
    "- input_tokens - The number of input tokens that were used.\n",
    "- output_tokens - The number of output tokens that were used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations\n",
    "\n",
    "\n",
    "A well-designed prompt evaluation consists of four primary components:\n",
    "\n",
    "Example Input: This is the instruction or question given to the model. It's crucial to design prompts that accurately represent the kinds of inputs your application will encounter in real-world use.\n",
    "Golden Answer: The correct or ideal response serves as a benchmark for the model's output. Creating high-quality golden answers often requires input from subject matter experts to ensure accuracy and relevance.\n",
    "Model Output: This is the actual response generated by the LLM based on the input prompt. It's what you'll be evaluating against the golden answer.\n",
    "Score: A quantitative or qualitative value representing the model's performance on that particular input. The scoring method can vary depending on the nature of your task and the grading approach you choose.\n",
    "\n",
    "\n",
    "![](./imdges/evals.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key prompting tips\n",
    "\n",
    "\n",
    "- Use the Prompt Generator\n",
    "- Be clear and direct\n",
    "- Use XML tags\n",
    "- Use examples (multishot prompting)\n",
    "- Let Claude think (chain of thought prompting)\n",
    "- Give Claude a role (system prompts)\n",
    "- Long context tips\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool\n",
    "\n",
    "格式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_definition = {\n",
    "    \"name\": \"get_stock_price\",\n",
    "    \"description\": \"Retrieves the current stock price for a given company\",\n",
    "    \"input_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"company\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The company name to fetch stock data for\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"company\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def process_tool_call(tool_name, tool_input):\n",
    "    \"\"\"\n",
    "    Process a tool call and return the result.\n",
    "    \"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"How many shares of General Motors can I buy with $500?\"}]\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-opus-20240229\",\n",
    "    messages= messages,\n",
    "    max_tokens=500,\n",
    "    tools=[tool_definition]\n",
    ")\n",
    "\n",
    "# Update messages to include Claude's response\n",
    "messages.append(\n",
    "    {\"role\": \"assistant\", \"content\": response.content}\n",
    ")\n",
    "\n",
    "#If Claude stops because it wants to use a tool:\n",
    "if response.stop_reason == \"tool_use\":\n",
    "    tool_use = response.content[-1] #Naive approach assumes only 1 tool is called at a time\n",
    "    tool_name = tool_use.name\n",
    "    tool_input = tool_use.input\n",
    "    print(f\"======Claude wants to use the {tool_name} tool======\")\n",
    "    #Actually run the underlying tool functionality on our db\n",
    "    tool_result = process_tool_call(tool_name, tool_input)\n",
    "    #Add our tool_result message:\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"tool_result\",\n",
    "                    \"tool_use_id\": tool_use.id,\n",
    "                    \"content\": str(tool_result),\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
