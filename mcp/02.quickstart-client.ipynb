{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Client Developers\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Your Environment\n",
    "\n",
    "\n",
    "```bash\n",
    "\n",
    "conda activate\n",
    "conda activate llm-study \n",
    "\n",
    "# langchain_community\n",
    "pip install mcp anthropic python-dotenv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Components Explained\n",
    "\n",
    "1. Client Initialization\n",
    " \n",
    "- The MCPClient class initializes with session management and API clients\n",
    "- Uses AsyncExitStack for proper resource management\n",
    "- Configures the Anthropic client for Claude interactions\n",
    "\n",
    "\n",
    "2. Server Connection\n",
    "\n",
    "- Supports both Python and Node.js servers\n",
    "- Validates server script type\n",
    "- Sets up proper communication channels\n",
    "- Initializes the session and lists available tools\n",
    "\n",
    "3. Query Processing\n",
    "\n",
    "- Maintains conversation context\n",
    "- Handles Claude’s responses and tool calls\n",
    "- Manages the message flow between Claude and tools\n",
    "- Combines results into a coherent response\n",
    "\n",
    "4. Interactive Interface\n",
    "    \n",
    "- Provides a simple command-line interface\n",
    "- Handles user input and displays responses\n",
    "- Includes basic error handling\n",
    "- Allows graceful exit\n",
    "\n",
    "\n",
    "5. Resource Management\n",
    "\n",
    "- Proper cleanup of resources\n",
    "- Error handling for connection issues\n",
    "- Graceful shutdown procedures\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from typing import Optional\n",
    "from contextlib import AsyncExitStack\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "from anthropic import Anthropic  # Claude, 可以替换为本地大模型 llama\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "\n",
    "load_dotenv()  # load environment variables from .env\n",
    "\n",
    "\n",
    "\n",
    "class MCPClient:\n",
    "    def __init__(self):\n",
    "        # Initialize session and client objects\n",
    "        self.session: Optional[ClientSession] = None\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.anthropic = Anthropic()\n",
    "        self.llm = Ollama(model=\"qwen2:7b\", request_timeout=30.0)\n",
    "    # methods will go here\n",
    "\n",
    "    async def connect_to_server(self, server_script_path: str):\n",
    "        \"\"\"Connect to an MCP server\n",
    "\n",
    "        Args:\n",
    "            server_script_path: Path to the server script (.py or .js)\n",
    "        \"\"\"\n",
    "        is_python = server_script_path.endswith('.py')\n",
    "        is_js = server_script_path.endswith('.js')\n",
    "        if not (is_python or is_js):\n",
    "            raise ValueError(\"Server script must be a .py or .js file\")\n",
    "\n",
    "        command = \"python\" if is_python else \"node\"\n",
    "        server_params = StdioServerParameters(\n",
    "            command=command,\n",
    "            args=[server_script_path],\n",
    "            env=None\n",
    "        )\n",
    "\n",
    "        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))\n",
    "        self.stdio, self.write = stdio_transport\n",
    "        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))\n",
    "\n",
    "        await self.session.initialize()\n",
    "\n",
    "        # List available tools\n",
    "        response = await self.session.list_tools()\n",
    "        tools = response.tools\n",
    "        print(\"\\nConnected to server with tools:\", [tool.name for tool in tools])\n",
    "\n",
    "    async def process_query(self, query: str) -> str:\n",
    "        \"\"\"Process a query using Claude and available tools\"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        response = await self.session.list_tools()\n",
    "        available_tools = [{\n",
    "            \"name\": tool.name,\n",
    "            \"description\": tool.description,\n",
    "            \"input_schema\": tool.inputSchema\n",
    "        } for tool in response.tools]\n",
    "\n",
    "        # Initial Claude API call\n",
    "        response = self.anthropic.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=1000,\n",
    "            messages=messages,\n",
    "            tools=available_tools\n",
    "        )\n",
    "\n",
    "        # Process response and handle tool calls\n",
    "        final_text = []\n",
    "\n",
    "        assistant_message_content = []\n",
    "        for content in response.content:\n",
    "            if content.type == 'text':\n",
    "                final_text.append(content.text)\n",
    "                assistant_message_content.append(content)\n",
    "            elif content.type == 'tool_use':\n",
    "                tool_name = content.name\n",
    "                tool_args = content.input\n",
    "\n",
    "                # Execute tool call\n",
    "                result = await self.session.call_tool(tool_name, tool_args)\n",
    "                final_text.append(f\"[Calling tool {tool_name} with args {tool_args}]\")\n",
    "\n",
    "                assistant_message_content.append(content)\n",
    "                messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": assistant_message_content\n",
    "                })\n",
    "                messages.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"tool_result\",\n",
    "                            \"tool_use_id\": content.id,\n",
    "                            \"content\": result.content\n",
    "                        }\n",
    "                    ]\n",
    "                })\n",
    "\n",
    "                # Get next response from Claude\n",
    "                response = self.anthropic.messages.create(\n",
    "                    model=\"claude-3-5-sonnet-20241022\",\n",
    "                    max_tokens=1000,\n",
    "                    messages=messages,\n",
    "                    tools=available_tools\n",
    "                )\n",
    "\n",
    "                final_text.append(response.content[0].text)\n",
    "\n",
    "        return \"\\n\".join(final_text)\n",
    "\n",
    "    async def chat_loop(self):\n",
    "        \"\"\"Run an interactive chat loop\"\"\"\n",
    "        print(\"\\nMCP Client Started!\")\n",
    "        print(\"Type your queries or 'quit' to exit.\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"\\nQuery: \").strip()\n",
    "\n",
    "                if query.lower() == 'quit':\n",
    "                    break\n",
    "\n",
    "                response = await self.process_query(query)\n",
    "                print(\"\\n\" + response)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError: {str(e)}\")\n",
    "\n",
    "    async def cleanup(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        await self.exit_stack.aclose()\n",
    "\n",
    "async def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Usage: python client.py <path_to_server_script>\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    client = MCPClient()\n",
    "    try:\n",
    "        await client.connect_to_server(sys.argv[1])\n",
    "        await client.chat_loop()\n",
    "    finally:\n",
    "        await client.cleanup()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Client\n",
    "\n",
    "\n",
    "```\n",
    "python client.py path/to/server.py # python server\n",
    "```\n",
    "\n",
    "\n",
    "The client will:\n",
    "\n",
    "- Connect to the specified server\n",
    "- List available tools\n",
    "- Start an interactive chat session where you can:\n",
    "    * Enter queries\n",
    "    * See tool executions\n",
    "    * Get responses from Claude\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "When you submit a query:\n",
    "\n",
    "- The client gets the list of available tools from the server\n",
    "- Your query is sent to Claude along with tool descriptions\n",
    "- Claude decides which tools (if any) to use\n",
    "- The client executes any requested tool calls through the server\n",
    "- Results are sent back to Claude\n",
    "- Claude provides a natural language response\n",
    "- The response is displayed to you\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best practices\n",
    "\n",
    "Error Handling\n",
    "\n",
    "- Always wrap tool calls in try-catch blocks\n",
    "- Provide meaningful error messages\n",
    "- Gracefully handle connection issues\n",
    "\n",
    "\n",
    "Resource Management\n",
    "\n",
    "- Use AsyncExitStack for proper cleanup\n",
    "- Close connections when done\n",
    "- Handle server disconnections\n",
    "\n",
    "Security\n",
    "\n",
    "- Store API keys securely in .env\n",
    "- Validate server responses\n",
    "- Be cautious with tool permissions\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
