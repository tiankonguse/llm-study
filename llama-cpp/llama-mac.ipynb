{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MacOS Install with Metal GPU\n",
    "\n",
    "(1) Make sure you have xcode installed... at least the command line parts\n",
    "\n",
    "```bash\n",
    "# check the path of your xcode install \n",
    "xcode-select -p\n",
    "\n",
    "# xcode installed returns\n",
    "# /Applications/Xcode-beta.app/Contents/Developer\n",
    "\n",
    "# if xcode is missing then install it... it takes ages;\n",
    "xcode-select --install\n",
    "```\n",
    "\n",
    "(2) Install the conda version for MacOS that supports Metal GPU\n",
    "\n",
    "```\n",
    "wget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh\n",
    "bash Miniforge3-MacOSX-arm64.sh\n",
    "```\n",
    "\n",
    "(3) Make a conda environment\n",
    "\n",
    "```\n",
    "conda create -n llama python=3.9.16\n",
    "conda activate llama\n",
    "```\n",
    "\n",
    "(4) Install the LATEST llama-cpp-python...which happily supports MacOS Metal GPU as of version 0.1.62\n",
    "\n",
    "```\n",
    "pip uninstall llama-cpp-python -y\n",
    "CMAKE_ARGS=\"-DGGML_METAL=on\" pip install -U llama-cpp-python --no-cache-dir\n",
    "pip install 'llama-cpp-python[server]'\n",
    "\n",
    "# you should now have llama-cpp-python v0.1.62 or higher installed\n",
    "pip list | grep llama_cpp_python\n",
    "# llama_cpp_python                         0.3.8\n",
    "```\n",
    "\n",
    "\n",
    "(5) Download a v3 gguf v2 model\n",
    "\n",
    "```\n",
    "huggingface-cli download  TheBloke/CodeLlama-7B-GGUF --local-dir TheBloke-CodeLlama-7B-GGUF\n",
    "```\n",
    "\n",
    "\n",
    "(6) run the llama-cpp-python API server with MacOS Metal GPU support\n",
    "\n",
    "\n",
    "```\n",
    "# config your ggml model path\n",
    "export MODEL=[path to your llama.cpp ggml models]]/[ggml-model-name]]Q4_0.gguf\n",
    "python -m llama_cpp.server --model $MODEL  --n_gpu_layers 1\n",
    "```\n",
    "\n",
    "python -m llama_cpp.server  --model /Users/tiankonguse-m3/models/qwq-32b.gguf  --port 8081"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
