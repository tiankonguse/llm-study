{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama 自定义导入模型\n",
    "\n",
    "## 从 GGUF 导入\n",
    "\n",
    "\n",
    "GGUF (GPT-Generated Unified Format) 是一种文件格式，用于保存经过微调的语言模型。  \n",
    "这种格式旨在帮助用户方便地在不同的平台和环境之间共享和导入模型。  \n",
    "它支持多种量化格式，可以有效减少模型文件的大小。  \n",
    "\n",
    "\n",
    "它的前身是 GGML(GPT-Generated Model Language)，是专门为了机器学习而设计的 Tensor 库，目的是为了有一个单文件的格式，并且易在不同架构的 CPU 以及 GPU 上可以推理，但后续由于开发遇到了灵活性不足、相容性及难以维护的问题。\n",
    "\n",
    "\n",
    "### 下载 .gguf 文件\n",
    "\n",
    "\n",
    "在 huggingface 上找到具体的 gguf 文件，点击下载  \n",
    "\n",
    "例如： https://huggingface.co/RichardErkhov/Qwen_-_Qwen2-0.5B-gguf\n",
    "\n",
    "### 新建创建 Modelfile 文件\n",
    "\n",
    "\n",
    "```\n",
    "FROM ./Qwen2-0.5B.Q3_K_M.gguf\n",
    "\n",
    "```\n",
    "\n",
    "### 在 Ollama 中创建模型\n",
    "\n",
    "\n",
    "```bash\n",
    "ollama create Qwen2-0.5B -f Modelfile\n",
    "```\n",
    "\n",
    "### 终端内运行模型\n",
    "\n",
    "```bash\n",
    "ollama run Qwen2-0.5B\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从 Pytorch 或 Safetensors 导入\n",
    "\n",
    "\n",
    "\n",
    "Safetensors 是一种用于存储深度学习模型权重的文件格式，它旨在解决安全性、效率和易用性方面的问题。目前这部分功能还有待社区成员开发，目前文档资源有限。\n",
    "\n",
    "\n",
    "\n",
    "### 下载模型\n",
    "\n",
    "安装依赖 `pip install huggingface_hub`  \n",
    "https://huggingface.co/docs/huggingface_hub/main/en/guides/download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tiankonguse-m3/miniconda3/envs/llm-study/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/tiankonguse-m3/miniconda3/envs/llm-study/lib/python3.12/site-packages/huggingface_hub/file_download.py:834: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00,  9.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/tiankonguse-m3/project/github/llm-study/ollama/Llama-3.2-1B-bnb-4bit'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下载模型\n",
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# 获取地址: https://huggingface.co/settings/tokens\n",
    "HUGGING_FACE_API_KEY = os.environ['HUGGING_FACE_API_KEY']\n",
    "\n",
    "model_id = \"unsloth/Llama-3.2-1B-bnb-4bit\"\n",
    "snapshot_download(\n",
    "  repo_id=model_id, \n",
    "  local_dir=\"Llama-3.2-1B-bnb-4bit\",\n",
    "  local_dir_use_symlinks=False,\n",
    "  revision=\"main\",\n",
    "  use_auth_token=HUGGING_FACE_API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建 Modelfile 文件\n",
    "\n",
    "```\n",
    "FROM ./llama-3-8b-bnb-4bit\n",
    "```\n",
    "\n",
    "### 创建模型\n",
    "\n",
    "```bash\n",
    "# Create the model in Ollama\n",
    "ollama create llama-3-8b-bnb-4bit -f Modelfile\n",
    "```\n",
    "\n",
    "### 运行模型\n",
    "\n",
    "```bash\n",
    "# Run the model\n",
    "ollama run llama-3-8b-bnb-4bit\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型直接导入\n",
    "\n",
    "\n",
    "正常来说，我们在 HuggingFace 接触到的模型文件非常之多，庆幸的是，hf 提供了非常便捷的 API 来下载和处理这些模型，像上面那样直接下载受限于网络环境，速度非常慢，这一小段我们来使用脚本以及 hf 来完成。\n",
    "\n",
    "llama.cpp 是 GGUF 的开源项目，提供 CLI 和 Server 功能。\n",
    "\n",
    "对于不能通过 Ollama 直接转换的架构，我们可以使用 llama.cpp 进行量化，并将其转换为 GGUF 格式，再按照第一种方式进行导入。 我们整个转换的过程分为以下几步：\n",
    "\n",
    "- 从 huggingface 上下载 model；\n",
    "- 使用 llama.cpp 来进行转化；\n",
    "- 使用 llama.cpp 来进行模型量化；\n",
    "- 运行并上传模型。\n",
    "\n",
    "\n",
    "### 从 HuggingFace 下载 Model\n",
    "\n",
    "\n",
    "最直觉的下载方式是通过git clone或者链接来下载，但是因为llm每部分都按GB计算，避免出现 OOM Error(Out of memory)，我们可以使用 Python 写一个简单的 download.py。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 10 files: 100%|██████████| 10/10 [05:11<00:00, 31.16s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/tiankonguse-m3/.cache/huggingface/hub/models--Qwen--Qwen1.5-0.5B/snapshots/8f445e3628f3500ee69f24e1303c9f10f5342a39'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下载模型\n",
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# 获取地址: https://huggingface.co/settings/tokens\n",
    "HUGGING_FACE_API_KEY = os.environ['HUGGING_FACE_API_KEY']\n",
    "\n",
    "model_id = \"Qwen/Qwen1.5-0.5B\" # hugginFace's model name\n",
    "snapshot_download(\n",
    "    repo_id=model_id, \n",
    "    # local_dir=\"Qwen-0.5b\",\n",
    "    # local_dir_use_symlinks=False,\n",
    "    revision=\"main\",\n",
    "    use_auth_token=HUGGING_FACE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 llama.cpp 进行转换\n",
    "\n",
    "lama.cpp 是 GGML 主要作者基于最早的 llama 的 c/c++ 版本开发的，目的就是希望用 CPU 来推理各种 LLM，在社群的不断努力下现在已经支持大多数主流模型，甚至包括多模态模型。\n",
    "\n",
    "首先我们克隆 llama.cpp 库到本地，与下载的模型放在同一目录下：\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/ggerganov/llama.cpp.git\n",
    "```\n",
    "\n",
    "由于使用 llama.cpp 转换模型的流程基于 python 开发，需要安装相关的库，推荐使用 conda 或 venv 新建一个环境。\n",
    "\n",
    "```bash\n",
    "cd llama.cpp\n",
    "pip install -r requirements.txt\n",
    "python convert_hf_to_gguf.py -h\n",
    "```\n",
    "\n",
    "接下来，我们把刚刚从 HuggingFace 下载的模型转换为 GGUF 格式，具体使用以下脚本：\n",
    "\n",
    "```bash\n",
    "python convert_hf_to_gguf.py ./Qwen-0.5b --outfile Qwen_instruct_0.5b.gguf --outtype f16\n",
    "\n",
    "```\n",
    "\n",
    "可以看到 llama.cpp 目录下多了一个 Qwen_instruct_0.5b.gguf 文件，这个过程只需要几秒钟。\n",
    "\n",
    "为了节省推理时的开销，我们将模型量化，接下来我们开始量化实操。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 使用llama.cpp进行模型量化\n",
    "\n",
    "\n",
    "模型量化是一种技术，将高精度的浮点数模型转换为低精度模型，模型量化的主要目的是减少模型的大小和计算成本，尽可能保持模型的准确性，其目标是使模型能够在资源有限的设备上运行，例如CPU或者移动设备。\n",
    "\n",
    "同样的，我们先创建 Modelfile 文件，再使 用 ollama create 命令来从 gguf 文件中创建我们的模型，不过与第一步稍有不同的是，我们添加了量化逻辑，只需要在执行 ollama create 是添加一个参数即可。\n",
    "\n",
    "\n",
    "首先把上一步拿到的 Qwen_instruct_0.5b.gguf 移动至第三部分的根目录下，再创建 Modelfile 文件编写以下内容.\n",
    "\n",
    "```\n",
    "FROM ./Qwen_instruct_0.5b.gguf\n",
    "```\n",
    "\n",
    "终端运行创建和量化脚本。\n",
    "\n",
    "```\n",
    "ollama create -q Q4_K_M mymodel3 -f ./Modelfile\n",
    "```\n",
    "\n",
    "### 上传模型\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os\n",
    "\n",
    "api = HfApi()\n",
    "HF_ACCESS_TOKEN = \"<YOUR_HF_WRITE_ACCESS_TOKEN>\"\n",
    "#TODO 这里需要设置你的model_id\n",
    "#例如 model_id = \"little1d/QWEN-0.5b\"\n",
    "model_id = \"your_hf_name/QWEN-0.5b\"\n",
    "\n",
    "\n",
    "api.create_repo(\n",
    "    model_id,\n",
    "    exist_ok=True,\n",
    "    repo_type=\"model\", # 上傳格式為模型\n",
    "    use_auth_token=HF_ACCESS_TOKEN,\n",
    ")\n",
    "# upload the model to the hub\n",
    "# upload model name includes the Bailong-instruct-7B in same folder\n",
    "for file in os.listdir():\n",
    "    if file.endswith(\".gguf\"):\n",
    "        model_name = file.lower()\n",
    "        api.upload_file(\n",
    "            repo_id=model_id,\n",
    "            path_in_repo=model_name,\n",
    "            path_or_fileobj=f\"{os.getcwd()}/{file}\",\n",
    "            repo_type=\"model\", # 上傳格式為模型\n",
    "            use_auth_token=HF_ACCESS_TOKE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义 Prompt\n",
    "\n",
    "Ollama 支持自定 义Prompt，可以让模型生成更符合用户需求的文本。\n",
    "\n",
    "\n",
    "### 根目录下创建一个 Modelfile 文件\n",
    "\n",
    "```Modelfile\n",
    "FROM llama3.1\n",
    "# sets the temperature to 1 [higher is more creative, lower is more coherent]\n",
    "PARAMETER temperature 1\n",
    "# sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token\n",
    "PARAMETER num_ctx 4096\n",
    "\n",
    "# sets a custom system message to specify the behavior of the chat assistant\n",
    "SYSTEM You are Mario from super mario bros, acting as an assistant.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### 创建模型\n",
    "\n",
    "```\n",
    "ollama create mymodel3 -f ./Modelfile\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
