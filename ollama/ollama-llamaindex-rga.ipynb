{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ LlamaIndex æ­å»ºæœ¬åœ° RAG åº”ç”¨\n",
    "\n",
    "https://github.com/datawhalechina/handy-ollama/tree/main/notebook/C7/LlamaIndex_RAG\n",
    "\n",
    "æœ¬æ–‡æ¡£å°†è¯¦ç»†ä»‹ç»å¦‚ä½•ä½¿ç”¨ LlamaIndex æ¡†æ¶æ¥æ­å»ºæœ¬åœ° RAGï¼ˆRetrieval-Augmented Generationï¼‰åº”ç”¨ã€‚  \n",
    "é€šè¿‡é›†æˆ LlamaIndexï¼Œå¯ä»¥åœ¨æœ¬åœ°ç¯å¢ƒä¸­æ„å»ºä¸€ä¸ª RAG ç³»ç»Ÿï¼Œç»“åˆæ£€ç´¢ä¸ç”Ÿæˆçš„èƒ½åŠ›ï¼Œä»¥æé«˜ä¿¡æ¯æ£€ç´¢çš„æ•ˆç‡å’Œç”Ÿæˆå†…å®¹çš„ç›¸å…³æ€§ã€‚  \n",
    "å¯ä»¥è‡ªå®šä¹‰æœ¬åœ°çŸ¥è¯†åº“è·¯å¾„ï¼Œé€šè¿‡ LlamaIndex æ„å»ºç´¢å¼•ï¼Œç„¶ååˆ©ç”¨ç´¢å¼•è¿›è¡Œä¸Šä¸‹æ–‡å¯¹è¯ã€‚\n",
    "\n",
    "\n",
    "\n",
    "## æ¨¡å‹ä¸‹è½½\n",
    "\n",
    "æœ¬ä¾‹ä¸­ä½¿ç”¨çš„æ˜¯ llama3.1 æ¨¡å‹ï¼Œå¯ä»¥æ ¹æ®è‡ªèº«ç”µè„‘é…ç½®ï¼Œä½¿ç”¨åˆé€‚çš„æ¨¡å‹ã€‚\n",
    "\n",
    "\n",
    "https://github.com/ollama/ollama\n",
    "\n",
    "\n",
    "```bash\n",
    "brew install ollama\n",
    "ollama pull llama3.2\n",
    "ollama pull nomic-embed-text\n",
    "\n",
    "conda activate llm-study\n",
    "pip install llama-index-llms-ollama\n",
    "pip install llama-index-embeddings-ollama\n",
    "pip install llama-index-readers-file\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŠ è½½æ•°æ®&æ„å»ºç´¢å¼•\n",
    "\n",
    "åŠ è½½å½“å‰ç›®å½•ä¸‹ data æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰çš„æ–‡æ¡£ï¼Œå¹¶åŠ è½½åˆ°å†…å­˜ä¸­ã€‚\n",
    "\n",
    "- Settings.embed_model ï¼š å…¨å±€çš„ embed_model å±æ€§ã€‚ç¤ºä¾‹ä»£ç ä¸­å°†åˆ›å»ºå¥½çš„åµŒå…¥æ¨¡å‹èµ‹å€¼ç»™å…¨å±€çš„ embed_model å±æ€§ï¼›\n",
    "- Settings.llm ï¼š å…¨å±€çš„ llm å±æ€§ã€‚ç¤ºä¾‹ä»£ç ä¸­å°†åˆ›å»ºå¥½çš„è¯­è¨€æ¨¡å‹èµ‹å€¼ç»™å…¨å±€çš„ llm å±æ€§ï¼›\n",
    "- VectorStoreIndex.from_documentsï¼šä½¿ç”¨ä¹‹å‰åŠ è½½çš„æ–‡æ¡£æ„å»ºç´¢å¼•ï¼Œå¹¶è½¬æ¢æˆå‘é‡ï¼Œä¾¿äºå¿«é€Ÿæ£€ç´¢ã€‚\n",
    "\n",
    "é€šè¿‡ Settings å…¨å±€å±æ€§çš„è®¾ç½®ï¼Œåœ¨åé¢çš„ç´¢å¼•æ„å»ºä»¥åŠæŸ¥è¯¢çš„è¿‡ç¨‹ä¸­å°±ä¼šé»˜è®¤ä½¿ç”¨ç›¸åº”çš„æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# nomic embedding model\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "# ollama\n",
    "Settings.llm = Ollama(model=\"llama3.2\", request_timeout=360.0)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æŸ¥è¯¢æ•°æ®\n",
    "\n",
    "index.as_query_engine()ï¼šæ ¹æ®ä¹‹å‰æ„å»ºå¥½çš„ç´¢å¼•ï¼Œåˆ›å»ºæŸ¥è¯¢å¼•æ“ã€‚è¯¥æŸ¥è¯¢å¼•æ“å¯ä»¥æ¥æ”¶æŸ¥è¯¢ï¼Œè¿”å›æ£€ç´¢åçš„å“åº”ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datawhaleæ˜¯ä¸€ä¸ªå¼€æºç»„ç»‡ã€‚\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥è¯¢æ•°æ®\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Datawhaleæ˜¯ä»€ä¹ˆ?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ£€ç´¢ä¸Šä¸‹æ–‡è¿›è¡Œå¯¹è¯\n",
    "\n",
    "ç”±äºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å¯èƒ½ä¼šå ç”¨å¤§é‡å¯ç”¨çš„ LLM ä¸Šä¸‹æ–‡ï¼Œå› æ­¤éœ€è¦ä¸ºèŠå¤©å†å²è®°å½•é…ç½®è¾ƒå° token é™åˆ¶ï¼\n",
    "\n",
    "chat_mode å¯ä»¥æ ¹æ®ä½¿ç”¨åœºæ™¯ï¼Œé€‰æ‹©åˆé€‚çš„æ¨¡å¼ï¼Œæ”¯æŒçš„æ¨¡å¼å¦‚ä¸‹ï¼š\n",
    "\n",
    "- bestï¼ˆé»˜è®¤ï¼‰ï¼šä½¿ç”¨å¸¦æœ‰æŸ¥è¯¢å¼•æ“å·¥å…·çš„ä»£ç†ï¼ˆreact æˆ– openaiï¼‰ï¼›\n",
    "- contextï¼šä½¿ç”¨æ£€ç´¢å™¨è·å–ä¸Šä¸‹æ–‡ï¼›\n",
    "- condense_questionï¼šå°†é—®é¢˜è¿›è¡Œæµ“ç¼©ï¼›\n",
    "- condense_plus_contextï¼šå°†é—®é¢˜è¿›è¡Œæµ“ç¼©å¹¶ä½¿ç”¨æ£€ç´¢å™¨è·å–ä¸Šä¸‹æ–‡ï¼›\n",
    "- simpleï¼šç›´æ¥ä½¿ç”¨ LLM çš„ç®€å•èŠå¤©å¼•æ“ï¼›\n",
    "- reactï¼šä½¿ç”¨å¸¦æœ‰æŸ¥è¯¢å¼•æ“å·¥å…·çš„ react ä»£ç†ï¼›\n",
    "- openaiï¼šä½¿ç”¨å¸¦æœ‰æŸ¥è¯¢å¼•æ“å·¥å…·çš„ openai ä»£ç†ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datawhaleæ˜¯ä¸€ä¸ªä¸“æ³¨äºæ•°æ®ç§‘å­¦ä¸ AI é¢†åŸŸçš„å¼€æºç»„ç»‡ï¼Œæˆç«‹äº2018å¹´ã€‚å®ƒèšé›†äº†æ¥è‡ªå¤šä¸ªé¢†åŸŸå’Œä¼ä¸šçš„ä¼˜ç§€å­¦ä¹ è€…å’Œå›¢é˜Ÿæˆå‘˜ï¼Œè‡´åŠ›äºæ„å»ºä¸€ä¸ªå¼€æ”¾ã€åŒ…å®¹çš„ç¤¾åŒºï¼Œä¸ºå­¦ä¹ è€…æä¾›æˆé•¿å’Œå‘å±•çš„æœºä¼šã€‚Datawhaleçš„ä½¿å‘½æ˜¯ \"for the learner, and learners together.\"\n"
     ]
    }
   ],
   "source": [
    "# æ£€ç´¢ä¸Šä¸‹æ–‡è¿›è¡Œå¯¹è¯\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
    "\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"context\",\n",
    "    memory=memory,\n",
    "    system_prompt=(\n",
    "        \"You are a chatbot, able to have normal interactions.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "response = chat_engine.chat(\"Datawhaleæ˜¯ä»€ä¹ˆï¼Ÿ\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datawhaleåœ¨2018å¹´æˆç«‹ã€‚\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"Datawhaleæ˜¯å“ªä¸€å¹´æˆç«‹çš„ï¼Ÿ\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datawhaleçš„ç›®æ ‡æ˜¯\"for the learnerï¼Œå’Œå­¦ä¹ è€…ä¸€èµ·æˆé•¿ã€‚\"è¿™æ„å‘³ç€å®ƒå…³å¿ƒçš„æ˜¯å­¦ä¹ è€…çš„æˆé•¿å’Œå‘å±•ï¼Œå®ƒä»¬å¸Œæœ›é€šè¿‡å¼€æºç¤¾åŒºã€å­¦ä¹ èµ„æºå’Œå¼€æ”¾æ€§æ¢ç´¢æ¥å¸®åŠ©å­¦ä¹ è€…åœ¨æ•°æ®ç§‘å­¦å’Œ AI é¢†åŸŸå–å¾—æˆå°±ã€‚\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"Datawhaleç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‘é‡ç´¢å¼•çš„å­˜å‚¨å’ŒåŠ è½½\n",
    "\n",
    "storage_context.persist å­˜å‚¨å‘é‡ç´¢å¼•ã€‚\n",
    "load_index_from_storage åŠ è½½å‘é‡ç´¢å¼•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å­˜å‚¨å‘é‡ç´¢å¼•\n",
    "persist_dir = 'data/'\n",
    "index.storage_context.persist(persist_dir=persist_dir)\n",
    "\n",
    "# åŠ è½½å‘é‡ç´¢å¼•\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "index= load_index_from_storage(storage_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## streamlit åº”ç”¨\n",
    "\n",
    "```\n",
    "pip install streamlit\n",
    "pip install llama_index\n",
    "pip install watchdog\n",
    "\n",
    "streamlit run ollama-llamaindex-app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 15:03:16.854 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.855 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.856 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.856 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.856 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.856 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.857 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.857 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.857 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.858 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.858 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.858 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.859 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.859 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.859 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.859 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.859 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.860 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.860 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.861 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.861 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.862 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.862 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.862 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.863 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.863 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.863 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.863 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.864 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.864 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.864 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.865 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.865 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.865 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.866 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.866 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.866 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.866 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.866 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.867 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.867 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.867 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "import os\n",
    "import tempfile\n",
    "import hashlib\n",
    "\n",
    "# OLLAMA_NUM_PARALLELï¼šåŒæ—¶å¤„ç†å•ä¸ªæ¨¡å‹çš„å¤šä¸ªè¯·æ±‚\n",
    "# OLLAMA_MAX_LOADED_MODELSï¼šåŒæ—¶åŠ è½½å¤šä¸ªæ¨¡å‹\n",
    "os.environ['OLLAMA_NUM_PARALLEL'] = '2'\n",
    "os.environ['OLLAMA_MAX_LOADED_MODELS'] = '2'\n",
    "\n",
    "\n",
    "# Function to handle file upload\n",
    "def handle_file_upload(uploaded_files):\n",
    "    if uploaded_files:\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        for uploaded_file in uploaded_files:\n",
    "            file_path = os.path.join(temp_dir, uploaded_file.name)\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(uploaded_file.getvalue())\n",
    "        return temp_dir\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function to calculate a hash for the uploaded files\n",
    "def get_files_hash(files):\n",
    "    hash_md5 = hashlib.md5()\n",
    "    for file in files:\n",
    "        file_bytes = file.read()\n",
    "        hash_md5.update(file_bytes)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "\n",
    "# Function to prepare generation configuration\n",
    "def prepare_generation_config():\n",
    "    with st.sidebar:\n",
    "        st.sidebar.header(\"Parameters\")\n",
    "        max_length = st.slider('Max Length', min_value=8, max_value=5080, value=4056)\n",
    "        temperature = st.slider('Temperature', 0.0, 1.0, 0.7, step=0.01)\n",
    "        st.button('Clear Chat History', on_click=clear_chat_history)\n",
    "\n",
    "    generation_config = {\n",
    "        'num_ctx': max_length,\n",
    "        'temperature': temperature\n",
    "    }\n",
    "    return generation_config\n",
    "\n",
    "\n",
    "# Function to clear chat history\n",
    "def clear_chat_history():\n",
    "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„åŠ©æ‰‹ï¼Œä½ éœ€è¦ä»€ä¹ˆå¸®åŠ©å—ï¼Ÿ\"}]\n",
    "\n",
    "\n",
    "# File upload in the sidebar\n",
    "st.sidebar.header(\"Upload Data\")\n",
    "uploaded_files = st.sidebar.file_uploader(\"Upload your data files:\", type=[\"txt\", \"pdf\", \"docx\"],\n",
    "                                          accept_multiple_files=True)\n",
    "\n",
    "generation_config = prepare_generation_config()\n",
    "\n",
    "\n",
    "# Function to initialize models\n",
    "@st.cache_resource\n",
    "def init_models():\n",
    "    embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "    Settings.embed_model = embed_model\n",
    "\n",
    "    llm = Ollama(model=\"llama3.2\", request_timeout=360.0,\n",
    "                 num_ctx=generation_config['num_ctx'],\n",
    "                 temperature=generation_config['temperature'])\n",
    "    Settings.llm = llm\n",
    "\n",
    "    documents = SimpleDirectoryReader(st.session_state['temp_dir']).load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "    memory = ChatMemoryBuffer.from_defaults(token_limit=4000)\n",
    "    chat_engine = index.as_chat_engine(\n",
    "        chat_mode=\"context\",\n",
    "        memory=memory,\n",
    "        system_prompt=\"You are a chatbot, able to have normal interactions.\",\n",
    "    )\n",
    "\n",
    "    return chat_engine\n",
    "\n",
    "\n",
    "# Streamlit application\n",
    "st.title(\"ğŸ’» Local RAG Chatbot ğŸ¤–\")\n",
    "st.caption(\"ğŸš€ A RAG chatbot powered by LlamaIndex and Ollama ğŸ¦™.\")\n",
    "\n",
    "# Initialize hash for the current uploaded files\n",
    "current_files_hash = get_files_hash(uploaded_files) if uploaded_files else None\n",
    "\n",
    "\n",
    "# Detect if files have changed and init models\n",
    "if 'files_hash' in st.session_state:\n",
    "    if st.session_state['files_hash'] != current_files_hash:\n",
    "        st.session_state['files_hash'] = current_files_hash\n",
    "        if 'chat_engine' in st.session_state:\n",
    "            del st.session_state['chat_engine']\n",
    "            st.cache_resource.clear()\n",
    "        if uploaded_files:\n",
    "            st.session_state['temp_dir'] = handle_file_upload(uploaded_files)\n",
    "            st.sidebar.success(\"Files uploaded successfully.\")\n",
    "            if 'chat_engine' not in st.session_state:\n",
    "                st.session_state['chat_engine'] = init_models()\n",
    "        else:\n",
    "            st.sidebar.error(\"No uploaded files.\")\n",
    "else:\n",
    "    if uploaded_files:\n",
    "        st.session_state['files_hash'] = current_files_hash\n",
    "        st.session_state['temp_dir'] = handle_file_upload(uploaded_files)\n",
    "        st.sidebar.success(\"Files uploaded successfully.\")\n",
    "        if 'chat_engine' not in st.session_state:\n",
    "            st.session_state['chat_engine'] = init_models()\n",
    "    else:\n",
    "        st.sidebar.error(\"No uploaded files.\")\n",
    "        \n",
    "\n",
    "# Initialize chat history\n",
    "if 'messages' not in st.session_state:\n",
    "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„åŠ©æ‰‹ï¼Œä½ éœ€è¦ä»€ä¹ˆå¸®åŠ©å—ï¼Ÿ\"}]\n",
    "\n",
    "# Display chat messages from history\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message['role'], avatar=message.get('avatar')):\n",
    "        st.markdown(message['content'])\n",
    "\n",
    "# Display chat input field at the bottom\n",
    "if prompt := st.chat_input(\"Ask a question about Datawhale:\"):\n",
    "\n",
    "    with st.chat_message('user'):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # Generate response\n",
    "    response = st.session_state['chat_engine'].stream_chat(prompt)\n",
    "    with st.chat_message('assistant'):\n",
    "        message_placeholder = st.empty()\n",
    "        res = ''\n",
    "        for token in response.response_gen:\n",
    "            res += token\n",
    "            message_placeholder.markdown(res + 'â–Œ')\n",
    "        message_placeholder.markdown(res)\n",
    "\n",
    "    # Add messages to history\n",
    "    st.session_state.messages.append({\n",
    "        'role': 'user',\n",
    "        'content': prompt,\n",
    "    })\n",
    "    st.session_state.messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': response,\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
