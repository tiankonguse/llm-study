{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 LlamaIndex 搭建本地 RAG 应用\n",
    "\n",
    "https://github.com/datawhalechina/handy-ollama/tree/main/notebook/C7/LlamaIndex_RAG\n",
    "\n",
    "本文档将详细介绍如何使用 LlamaIndex 框架来搭建本地 RAG（Retrieval-Augmented Generation）应用。  \n",
    "通过集成 LlamaIndex，可以在本地环境中构建一个 RAG 系统，结合检索与生成的能力，以提高信息检索的效率和生成内容的相关性。  \n",
    "可以自定义本地知识库路径，通过 LlamaIndex 构建索引，然后利用索引进行上下文对话。\n",
    "\n",
    "\n",
    "\n",
    "## 模型下载\n",
    "\n",
    "本例中使用的是 llama3.1 模型，可以根据自身电脑配置，使用合适的模型。\n",
    "\n",
    "\n",
    "https://github.com/ollama/ollama\n",
    "\n",
    "\n",
    "```bash\n",
    "brew install ollama\n",
    "ollama pull llama3.2\n",
    "ollama pull nomic-embed-text\n",
    "\n",
    "conda activate llm-study\n",
    "pip install llama-index-llms-ollama\n",
    "pip install llama-index-embeddings-ollama\n",
    "pip install llama-index-readers-file\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据&构建索引\n",
    "\n",
    "加载当前目录下 data 文件夹中所有的文档，并加载到内存中。\n",
    "\n",
    "- Settings.embed_model ： 全局的 embed_model 属性。示例代码中将创建好的嵌入模型赋值给全局的 embed_model 属性；\n",
    "- Settings.llm ： 全局的 llm 属性。示例代码中将创建好的语言模型赋值给全局的 llm 属性；\n",
    "- VectorStoreIndex.from_documents：使用之前加载的文档构建索引，并转换成向量，便于快速检索。\n",
    "\n",
    "通过 Settings 全局属性的设置，在后面的索引构建以及查询的过程中就会默认使用相应的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# nomic embedding model\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "# ollama\n",
    "Settings.llm = Ollama(model=\"llama3.2\", request_timeout=360.0)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 查询数据\n",
    "\n",
    "index.as_query_engine()：根据之前构建好的索引，创建查询引擎。该查询引擎可以接收查询，返回检索后的响应。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datawhale是一个开源组织。\n"
     ]
    }
   ],
   "source": [
    "# 查询数据\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Datawhale是什么?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检索上下文进行对话\n",
    "\n",
    "由于检索到的上下文可能会占用大量可用的 LLM 上下文，因此需要为聊天历史记录配置较小 token 限制！\n",
    "\n",
    "chat_mode 可以根据使用场景，选择合适的模式，支持的模式如下：\n",
    "\n",
    "- best（默认）：使用带有查询引擎工具的代理（react 或 openai）；\n",
    "- context：使用检索器获取上下文；\n",
    "- condense_question：将问题进行浓缩；\n",
    "- condense_plus_context：将问题进行浓缩并使用检索器获取上下文；\n",
    "- simple：直接使用 LLM 的简单聊天引擎；\n",
    "- react：使用带有查询引擎工具的 react 代理；\n",
    "- openai：使用带有查询引擎工具的 openai 代理。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datawhale是一个专注于数据科学与 AI 领域的开源组织，成立于2018年。它聚集了来自多个领域和企业的优秀学习者和团队成员，致力于构建一个开放、包容的社区，为学习者提供成长和发展的机会。Datawhale的使命是 \"for the learner, and learners together.\"\n"
     ]
    }
   ],
   "source": [
    "# 检索上下文进行对话\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
    "\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"context\",\n",
    "    memory=memory,\n",
    "    system_prompt=(\n",
    "        \"You are a chatbot, able to have normal interactions.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "response = chat_engine.chat(\"Datawhale是什么？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datawhale在2018年成立。\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"Datawhale是哪一年成立的？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datawhale的目标是\"for the learner，和学习者一起成长。\"这意味着它关心的是学习者的成长和发展，它们希望通过开源社区、学习资源和开放性探索来帮助学习者在数据科学和 AI 领域取得成就。\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"Datawhale目标是什么？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 向量索引的存储和加载\n",
    "\n",
    "storage_context.persist 存储向量索引。\n",
    "load_index_from_storage 加载向量索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存储向量索引\n",
    "persist_dir = 'data/'\n",
    "index.storage_context.persist(persist_dir=persist_dir)\n",
    "\n",
    "# 加载向量索引\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "index= load_index_from_storage(storage_context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## streamlit 应用\n",
    "\n",
    "```\n",
    "pip install streamlit\n",
    "pip install llama_index\n",
    "pip install watchdog\n",
    "\n",
    "streamlit run ollama-llamaindex-app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-22 15:03:16.854 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.855 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.856 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.856 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.856 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.856 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.857 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.857 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.857 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.858 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.858 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.858 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.859 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.859 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.859 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.859 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.859 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.860 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.860 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.861 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.861 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.862 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.862 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.862 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.863 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.863 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.863 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.863 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.864 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.864 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.864 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.865 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.865 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.865 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.866 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.866 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.866 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.866 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.866 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.867 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.867 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-22 15:03:16.867 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "import os\n",
    "import tempfile\n",
    "import hashlib\n",
    "\n",
    "# OLLAMA_NUM_PARALLEL：同时处理单个模型的多个请求\n",
    "# OLLAMA_MAX_LOADED_MODELS：同时加载多个模型\n",
    "os.environ['OLLAMA_NUM_PARALLEL'] = '2'\n",
    "os.environ['OLLAMA_MAX_LOADED_MODELS'] = '2'\n",
    "\n",
    "\n",
    "# Function to handle file upload\n",
    "def handle_file_upload(uploaded_files):\n",
    "    if uploaded_files:\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        for uploaded_file in uploaded_files:\n",
    "            file_path = os.path.join(temp_dir, uploaded_file.name)\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(uploaded_file.getvalue())\n",
    "        return temp_dir\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function to calculate a hash for the uploaded files\n",
    "def get_files_hash(files):\n",
    "    hash_md5 = hashlib.md5()\n",
    "    for file in files:\n",
    "        file_bytes = file.read()\n",
    "        hash_md5.update(file_bytes)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "\n",
    "# Function to prepare generation configuration\n",
    "def prepare_generation_config():\n",
    "    with st.sidebar:\n",
    "        st.sidebar.header(\"Parameters\")\n",
    "        max_length = st.slider('Max Length', min_value=8, max_value=5080, value=4056)\n",
    "        temperature = st.slider('Temperature', 0.0, 1.0, 0.7, step=0.01)\n",
    "        st.button('Clear Chat History', on_click=clear_chat_history)\n",
    "\n",
    "    generation_config = {\n",
    "        'num_ctx': max_length,\n",
    "        'temperature': temperature\n",
    "    }\n",
    "    return generation_config\n",
    "\n",
    "\n",
    "# Function to clear chat history\n",
    "def clear_chat_history():\n",
    "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"你好，我是你的助手，你需要什么帮助吗？\"}]\n",
    "\n",
    "\n",
    "# File upload in the sidebar\n",
    "st.sidebar.header(\"Upload Data\")\n",
    "uploaded_files = st.sidebar.file_uploader(\"Upload your data files:\", type=[\"txt\", \"pdf\", \"docx\"],\n",
    "                                          accept_multiple_files=True)\n",
    "\n",
    "generation_config = prepare_generation_config()\n",
    "\n",
    "\n",
    "# Function to initialize models\n",
    "@st.cache_resource\n",
    "def init_models():\n",
    "    embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "    Settings.embed_model = embed_model\n",
    "\n",
    "    llm = Ollama(model=\"llama3.2\", request_timeout=360.0,\n",
    "                 num_ctx=generation_config['num_ctx'],\n",
    "                 temperature=generation_config['temperature'])\n",
    "    Settings.llm = llm\n",
    "\n",
    "    documents = SimpleDirectoryReader(st.session_state['temp_dir']).load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "    memory = ChatMemoryBuffer.from_defaults(token_limit=4000)\n",
    "    chat_engine = index.as_chat_engine(\n",
    "        chat_mode=\"context\",\n",
    "        memory=memory,\n",
    "        system_prompt=\"You are a chatbot, able to have normal interactions.\",\n",
    "    )\n",
    "\n",
    "    return chat_engine\n",
    "\n",
    "\n",
    "# Streamlit application\n",
    "st.title(\"💻 Local RAG Chatbot 🤖\")\n",
    "st.caption(\"🚀 A RAG chatbot powered by LlamaIndex and Ollama 🦙.\")\n",
    "\n",
    "# Initialize hash for the current uploaded files\n",
    "current_files_hash = get_files_hash(uploaded_files) if uploaded_files else None\n",
    "\n",
    "\n",
    "# Detect if files have changed and init models\n",
    "if 'files_hash' in st.session_state:\n",
    "    if st.session_state['files_hash'] != current_files_hash:\n",
    "        st.session_state['files_hash'] = current_files_hash\n",
    "        if 'chat_engine' in st.session_state:\n",
    "            del st.session_state['chat_engine']\n",
    "            st.cache_resource.clear()\n",
    "        if uploaded_files:\n",
    "            st.session_state['temp_dir'] = handle_file_upload(uploaded_files)\n",
    "            st.sidebar.success(\"Files uploaded successfully.\")\n",
    "            if 'chat_engine' not in st.session_state:\n",
    "                st.session_state['chat_engine'] = init_models()\n",
    "        else:\n",
    "            st.sidebar.error(\"No uploaded files.\")\n",
    "else:\n",
    "    if uploaded_files:\n",
    "        st.session_state['files_hash'] = current_files_hash\n",
    "        st.session_state['temp_dir'] = handle_file_upload(uploaded_files)\n",
    "        st.sidebar.success(\"Files uploaded successfully.\")\n",
    "        if 'chat_engine' not in st.session_state:\n",
    "            st.session_state['chat_engine'] = init_models()\n",
    "    else:\n",
    "        st.sidebar.error(\"No uploaded files.\")\n",
    "        \n",
    "\n",
    "# Initialize chat history\n",
    "if 'messages' not in st.session_state:\n",
    "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"你好，我是你的助手，你需要什么帮助吗？\"}]\n",
    "\n",
    "# Display chat messages from history\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message['role'], avatar=message.get('avatar')):\n",
    "        st.markdown(message['content'])\n",
    "\n",
    "# Display chat input field at the bottom\n",
    "if prompt := st.chat_input(\"Ask a question about Datawhale:\"):\n",
    "\n",
    "    with st.chat_message('user'):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # Generate response\n",
    "    response = st.session_state['chat_engine'].stream_chat(prompt)\n",
    "    with st.chat_message('assistant'):\n",
    "        message_placeholder = st.empty()\n",
    "        res = ''\n",
    "        for token in response.response_gen:\n",
    "            res += token\n",
    "            message_placeholder.markdown(res + '▌')\n",
    "        message_placeholder.markdown(res)\n",
    "\n",
    "    # Add messages to history\n",
    "    st.session_state.messages.append({\n",
    "        'role': 'user',\n",
    "        'content': prompt,\n",
    "    })\n",
    "    st.session_state.messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': response,\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
