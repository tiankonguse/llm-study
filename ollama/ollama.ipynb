{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ollama 介绍\n",
    "\n",
    "Get up and running with large language models.\n",
    "\n",
    "\n",
    "https://ollama.com/\n",
    "https://github.com/ollama/ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UI 交互"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载 ollama\n",
    "\n",
    "\n",
    "https://ollama.com/download\n",
    "\n",
    "可以直接下载安装包，支持 windows、Mac、Linux 三端。\n",
    "\n",
    "\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/install.sh | bash\n",
    "\n",
    "ollama --version\n",
    "# output: ollama version is 0.5.11\n",
    "```\n",
    "\n",
    "\n",
    "Docker 安装\n",
    "\n",
    "```bash\n",
    "docker pull ollama/ollama\n",
    "docker run -p 11434:11434 ollama/ollama\n",
    "```\n",
    "\n",
    "访问 http://localhost:11434 即可使用 Ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载与运行模型\n",
    "\n",
    "You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.\n",
    "\n",
    "下载模型\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.2\n",
    "```\n",
    "\n",
    "\n",
    "运行模型\n",
    "\n",
    "注：如果运行模型时，模型尚未下载，则会自动下载，下载完成后自动运行模型。\n",
    "\n",
    "\n",
    "```bash\n",
    "ollama run llama3.2\n",
    "```\n",
    "\n",
    "运行模型后，就可以在命令行里一问一答的进行对话了。  \n",
    "\n",
    "\n",
    "\n",
    "```text\n",
    ">>> Send a message (/? for help)\n",
    ">>> /?\n",
    "Available Commands:\n",
    "  /set            Set session variables\n",
    "  /show           Show model information\n",
    "  /load <model>   Load a session or model\n",
    "  /save <model>   Save your current session\n",
    "  /clear          Clear session context\n",
    "  /bye            Exit\n",
    "  /?, /help       Help for a command\n",
    "  /? shortcuts    Help for keyboard shortcuts\n",
    "\n",
    "Use \"\"\" to begin a multi-line message.\n",
    "\n",
    ">>> who are you?\n",
    "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n",
    "\n",
    ">>> can you speek chinese?\n",
    "I can understand and generate text in Simplified Chinese, but my proficiency may not be as high as that of a native speaker or a professional translator.\n",
    "\n",
    "If you'd like to communicate in Chinese, I can try to:\n",
    "\n",
    "1. Understand and respond to simple questions or phrases\n",
    "2. Generate text in Simplified Chinese on various topics\n",
    "3. Translate English text into Simplified Chinese\n",
    "\n",
    "However, please note that my ability to understand nuances, idioms, and complex conversations may be limited.\n",
    "\n",
    "Which aspect of Chinese language would you like me to help with?\n",
    "```\n",
    "\n",
    "\n",
    "### Multiline input\n",
    "\n",
    "```text\n",
    ">>> \"\"\"Hello,\n",
    "... world!\n",
    "... \"\"\"\n",
    "I'm a basic program that prints the famous \"Hello, world!\" message to the console.\n",
    "```\n",
    "\n",
    "\n",
    "### Multimodal models\n",
    "\n",
    "\n",
    "```bash\n",
    "ollama run llava \"What's in this image? /Users/jmorgan/Desktop/smile.png\"\n",
    "```\n",
    "\n",
    "### Pass the prompt as an argument\n",
    "\n",
    "```bash\n",
    "ollama run llama3.2 \"Summarize this file: $(cat README.md)\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDK 交互\n",
    "\n",
    "https://github.com/ollama/ollama-python/tree/main/examples\n",
    "\n",
    "安装 python SDK\n",
    "\n",
    "\n",
    "## 安装 SDK\n",
    "\n",
    "```bash\n",
    "pip install ollama\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homePath:  /Users/tiankonguse\n",
      "basePath:  /Users/tiankonguse\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# python 获取系统变量 HOME 变量\n",
    "import os\n",
    "homePath = os.environ['HOME']\n",
    "print(\"homePath: \",homePath)\n",
    "\n",
    "# 修改成自己的 HOME 路径\n",
    "basePath=homePath\n",
    "\n",
    "print(\"basePath: \",basePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Who are you\n",
      "Response str: I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n",
      "\n",
      "prompt: What is your name\n",
      "Response str: I don't have a personal name. I'm an AI designed to assist and communicate with users, and I'm often referred to as a \"language model\" or a \"chatbot.\" My purpose is to provide information, answer questions, and engage in conversation to the best of my abilities, based on my training and knowledge.\n",
      "\n",
      "If you'd like, I can adopt a persona or nickname for our conversations. Some people call me \"Luna\" or \"Nova,\" but feel free to give me any name you'd like!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import shlex\n",
    "import ollama\n",
    "import json\n",
    "\n",
    "prompts = [\"Who are you\", \"What is your name\"]\n",
    "\n",
    "# Iterate over prompts and generate responses\n",
    "for prompt in prompts:\n",
    "    response = ollama.generate(\n",
    "        model=\"llama3.2\",  # 模型名称\n",
    "        prompt=prompt  # 提示文本\n",
    "    )\n",
    "\n",
    "    response = response.response\n",
    "\n",
    "    # 打印 response 的类型\n",
    "    # print(\"Response Type:\", type(response))\n",
    "\n",
    "\n",
    "    print(\"prompt:\", prompt)\n",
    "    # 如果 response 是 数组，使用 ”“ 连接数组\n",
    "    if isinstance(response, list):\n",
    "        print(\"Response list:\", \" \".join(response)) \n",
    "    elif isinstance(response, str):\n",
    "        print(\"Response str:\", response)\n",
    "    else:\n",
    "        print(\"Response other:\", response)\n",
    "    print(\"\")\n",
    "\n",
    "# prompt: Who are you\n",
    "# Response str: I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n",
    "\n",
    "# prompt: What is your name\n",
    "# Response str: I don't have a personal name, but I'm an AI designed to assist and communicate with users. You can think of me as a conversational AI or a chatbot. I'm here to help answer your questions, provide information, and engage in conversations to the best of my abilities. Is there something specific you'd like to talk about or ask?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对话模式(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "response = chat(\n",
    "    model=\"llama3.2\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Who are you?\"}\n",
    "    ]\n",
    ")\n",
    "# 转化为 json 格式化打印 response\n",
    "\n",
    "print(response.message.content)\n",
    "# I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 流式响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\""
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "stream = chat(\n",
    "    model=\"llama3.2\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Who are you?\"}],\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "# I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他 SDK API\n",
    "\n",
    "列出所有可用的模型 `ollama.list()`\n",
    "显示指定模型的详细信息 `ollama.show('llama3.2')`\n",
    "从远程仓库拉取模型 `ollama.pull('llama3.2')`\n",
    "生成文本嵌入 `ollama.embed(model='llama3.2', input='The sky is blue because of rayleigh scattering')`\n",
    "查看正在运行的模型列表 `ollama.ps()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fill-in-middle\n",
    "\n",
    "填充开头开结尾，自动补充中间\n",
    "\n",
    "\n",
    "使用场景：\n",
    "\n",
    "- 代码补全， 模型：codellama:7b-code\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sort the string s in ascending order \"\"\"\n",
      "    # Convert the string to a list of characters\n",
      "    char_list = list(s)\n",
      "    \n",
      "    # Sort the list of characters\n",
      "    char_list.sort()\n",
      "    \n",
      "    # Join the sorted list back into a string\n",
      "    result = ''.join(char_list)\n",
      "    \n",
      "    return result\n",
      "\n",
      "# Example usage:\n",
      "input_string = \"hello\"\n",
      "sorted_string = Sort(input_string)\n",
      "print(sorted_string)  # Output: \"ehllo\"  # The characters are sorted in ascending order\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ollama import generate\n",
    "\n",
    "prompt = '''def Sort(s: str) -> str:\n",
    "    \"\"\" '''\n",
    "\n",
    "suffix = \"\"\"\n",
    "    return result\n",
    "\"\"\"\n",
    "\n",
    "response = generate(\n",
    "  model='qwen2.5-coder:0.5b',\n",
    "  prompt=prompt,\n",
    "  suffix=suffix,\n",
    "  options={\n",
    "    'num_predict': 128,\n",
    "    'temperature': 0,\n",
    "    'top_p': 0.9,\n",
    "    'stop': ['<EOT>'],\n",
    "  },\n",
    ")\n",
    "\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## call function\n",
    "\n",
    "自定义插件函数，大模型预处理后，主动调研插件，结果再传给大模型，最后输出结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is 1 + 4 - 2 * 6?\n",
      "Calling function: add_two_numbers\n",
      "Arguments: {'a': '7', 'b': '4'}\n",
      "Function output: 11\n",
      "Calling function: subtract_two_numbers\n",
      "Arguments: {'a': '3', 'b': '-12'}\n",
      "Function output: 15\n",
      "Tool call result added to messages:  [{'role': 'user', 'content': 'What is 1 + 4 - 2 * 6?'}, Message(role='assistant', content='', images=None, tool_calls=[ToolCall(function=Function(name='add_two_numbers', arguments={'a': '7', 'b': '4'})), ToolCall(function=Function(name='subtract_two_numbers', arguments={'a': '3', 'b': '-12'}))]), {'role': 'tool', 'content': '15', 'name': 'subtract_two_numbers'}]\n",
      "Final response: To calculate the expression 1 + 4 - 2 * 6, we need to follow the order of operations (PEMDAS):\n",
      "\n",
      "1. Multiply 2 and 6: 2 * 6 = 12\n",
      "2. Add 1 and 4: 1 + 4 = 5\n",
      "3. Subtract 12 from 5: 5 - 12 = -7\n",
      "\n",
      "So, the final result is -7.\n"
     ]
    }
   ],
   "source": [
    "from ollama import ChatResponse, chat\n",
    "\n",
    "\n",
    "def add_two_numbers(a: int, b: int) -> int:\n",
    "  \"\"\"\n",
    "  Add two numbers\n",
    "\n",
    "  Args:\n",
    "    a (int): The first number\n",
    "    b (int): The second number\n",
    "\n",
    "  Returns:\n",
    "    int: The sum of the two numbers\n",
    "  \"\"\"\n",
    "\n",
    "  # The cast is necessary as returned tool call arguments don't always conform exactly to schema\n",
    "  # E.g. this would prevent \"what is 30 + 12\" to produce '3012' instead of 42\n",
    "  return int(a) + int(b)\n",
    "\n",
    "\n",
    "def subtract_two_numbers(a: int, b: int) -> int:\n",
    "  \"\"\"\n",
    "  Subtract two numbers\n",
    "  \"\"\"\n",
    "\n",
    "  # The cast is necessary as returned tool call arguments don't always conform exactly to schema\n",
    "  return int(a) - int(b)\n",
    "\n",
    "\n",
    "# Tools can still be manually defined and passed into chat\n",
    "subtract_two_numbers_tool = {\n",
    "  'type': 'function',\n",
    "  'function': {\n",
    "    'name': 'subtract_two_numbers',\n",
    "    'description': 'Subtract two numbers',\n",
    "    'parameters': {\n",
    "      'type': 'object',\n",
    "      'required': ['a', 'b'],\n",
    "      'properties': {\n",
    "        'a': {'type': 'integer', 'description': 'The first number'},\n",
    "        'b': {'type': 'integer', 'description': 'The second number'},\n",
    "      },\n",
    "    },\n",
    "  },\n",
    "}\n",
    "\n",
    "messages = [{'role': 'user', 'content': 'What is 1 + 4 - 2 * 6?'}]\n",
    "print('Prompt:', messages[0]['content'])\n",
    "\n",
    "available_functions = {\n",
    "  'add_two_numbers': add_two_numbers,\n",
    "  'subtract_two_numbers': subtract_two_numbers,\n",
    "}\n",
    "\n",
    "model = \"llama3.2\"\n",
    "\n",
    "response: ChatResponse = chat(\n",
    "  model,\n",
    "  messages=messages,\n",
    "  tools=[add_two_numbers, subtract_two_numbers_tool],\n",
    ")\n",
    "\n",
    "if response.message.tool_calls:\n",
    "  # There may be multiple tool calls in the response\n",
    "  for tool in response.message.tool_calls:\n",
    "    # Ensure the function is available, and then call it\n",
    "    if function_to_call := available_functions.get(tool.function.name):\n",
    "      print('Calling function:', tool.function.name)\n",
    "      print('Arguments:', tool.function.arguments)\n",
    "      output = function_to_call(**tool.function.arguments)\n",
    "      print('Function output:', output)\n",
    "    else:\n",
    "      print('Function', tool.function.name, 'not found')\n",
    "\n",
    "# Only needed to chat with the model using the tool call results\n",
    "if response.message.tool_calls:\n",
    "  # Add the function response to messages for the model to use\n",
    "  messages.append(response.message)\n",
    "  messages.append({'role': 'tool', 'content': str(output), 'name': tool.function.name})\n",
    "  \n",
    "  print('Tool call result added to messages: ', messages)\n",
    "  # Get final response from model with function outputs\n",
    "  final_response = chat(model, messages=messages)\n",
    "  print('Final response:', final_response.message.content)\n",
    "\n",
    "else:\n",
    "  print('No tool calls returned from model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chat with image\n",
    "\n",
    "对话时，带上图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m model = \u001b[33m'\u001b[39m\u001b[33mllama3.2-vision\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     14\u001b[39m model = \u001b[33m'\u001b[39m\u001b[33mllama3.2-vision:11b\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m response = \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m      \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m      \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mWhat is in this image? More details\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m      \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimages\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m  \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.message.content)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# You can also pass in base64 encoded image data\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm-study/lib/python3.12/site-packages/ollama/_client.py:333\u001b[39m, in \u001b[36mClient.chat\u001b[39m\u001b[34m(self, model, messages, tools, stream, format, options, keep_alive)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m    290\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    291\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    298\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    299\u001b[39m ) -> Union[ChatResponse, Iterator[ChatResponse]]:\n\u001b[32m    300\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[33;03m  Create a chat response using the requested model.\u001b[39;00m\n\u001b[32m    302\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    331\u001b[39m \u001b[33;03m  Returns `ChatResponse` if `stream` is `False`, otherwise returns a `ChatResponse` generator.\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mChatResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/chat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_copy_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm-study/lib/python3.12/site-packages/ollama/_client.py:178\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    176\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm-study/lib/python3.12/site-packages/ollama/_client.py:118\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_request_raw\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    117\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m     r.raise_for_status()\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm-study/lib/python3.12/site-packages/httpx/_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm-study/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm-study/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm-study/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm-study/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm-study/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm-study/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm-study/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm-study/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm-study/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm-study/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm-study/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm-study/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/llm-study/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "from ollama import chat\n",
    "\n",
    "# from pathlib import Path\n",
    "\n",
    "# Pass in the path to the image\n",
    "# path = input('Pass in the path to the image')\n",
    "path = basePath + 'project/github/ComfyUI/output/ComfyUI_00099_.png'\n",
    "path='/Users/tiankonguse/project/github/faceswap/photo/wyz_wbq/video-frame-1.png'\n",
    "\n",
    "model = 'llava'\n",
    "model = 'llama3.2-vision'\n",
    "model = 'llama3.2-vision:11b'\n",
    "\n",
    "response = chat(\n",
    "  model=model,\n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': 'What is in this image? More details',\n",
    "      'images': [path],\n",
    "    }\n",
    "  ],\n",
    ")\n",
    "\n",
    "print(response.message.content)\n",
    "\n",
    "\n",
    "# You can also pass in base64 encoded image data\n",
    "img = base64.b64encode(Path(path).read_bytes()).decode()\n",
    "# or the raw bytes\n",
    "# img = Path(path).read_bytes()\n",
    "\n",
    "response = chat(\n",
    "  model='llava',\n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': 'What is in this image? Can you describe it?',\n",
    "      'images': [img],\n",
    "    }\n",
    "  ],\n",
    ")\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate with image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "from ollama import generate\n",
    "\n",
    "# Pass in the path to the image\n",
    "path = basePath + '/project/github/ComfyUI/output/ComfyUI_00099_.png'\n",
    "img = base64.b64encode(Path(path).read_bytes()).decode()\n",
    "\n",
    "\n",
    "\n",
    "for response in generate('llava', 'explain this image:', images=[img], stream=True):\n",
    "  print(response['response'], end='', flush=True)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## structured-outputs-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from ollama import chat\n",
    "\n",
    "\n",
    "# Define the schema for image objects\n",
    "class Object(BaseModel):\n",
    "  name: str\n",
    "  confidence: float\n",
    "  attributes: str\n",
    "\n",
    "\n",
    "class ImageDescription(BaseModel):\n",
    "  summary: str\n",
    "  objects: list[Object]\n",
    "  scene: str\n",
    "  colors: list[str]\n",
    "  time_of_day: Literal['Morning', 'Afternoon', 'Evening', 'Night']\n",
    "  setting: Literal['Indoor', 'Outdoor', 'Unknown']\n",
    "  text_content: str | None = None\n",
    "\n",
    "\n",
    "# Get path from user input\n",
    "path = basePath + '/project/github/ComfyUI/output/ComfyUI_00099_.png'\n",
    "path = Path(path)\n",
    "\n",
    "# Verify the file exists\n",
    "if not path.exists():\n",
    "  raise FileNotFoundError(f'Image not found at: {path}')\n",
    "\n",
    "model = 'llava'\n",
    "\n",
    "# Set up chat as usual\n",
    "response = chat(\n",
    "  model=model,\n",
    "  format=ImageDescription.model_json_schema(),  # Pass in the schema for the response\n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': 'Analyze this image and return a detailed JSON description including objects, scene, colors and any text detected. If you cannot determine certain details, leave those fields empty.',\n",
    "      'images': [path],\n",
    "    },\n",
    "  ],\n",
    "  options={'temperature': 0},  # Set temperature to 0 for more deterministic output\n",
    ")\n",
    "\n",
    "\n",
    "# Convert received content to the schema\n",
    "image_analysis = ImageDescription.model_validate_json(response.message.content)\n",
    "\n",
    "# json 格式化输出, 分隔符 2个空格\n",
    "json_output = image_analysis.model_dump_json(indent=2)\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama 相关命令\n",
    "\n",
    "\n",
    "## ollama --help\n",
    "\n",
    "```bash\n",
    "ollama --help\n",
    "\n",
    "```\n",
    "\n",
    "```text\n",
    "Large language model runner\n",
    "\n",
    "Usage:\n",
    "  ollama [flags]\n",
    "  ollama [command]\n",
    "\n",
    "Available Commands:\n",
    "  serve       Start ollama\n",
    "  create      Create a model from a Modelfile\n",
    "  show        Show information for a model\n",
    "  run         Run a model\n",
    "  stop        Stop a running model\n",
    "  pull        Pull a model from a registry\n",
    "  push        Push a model to a registry\n",
    "  list        List models\n",
    "  ps          List running models\n",
    "  cp          Copy a model\n",
    "  rm          Remove a model\n",
    "  help        Help about any command\n",
    "\n",
    "Flags:\n",
    "  -h, --help      help for ollama\n",
    "  -v, --version   Show version information\n",
    "\n",
    "Use \"ollama [command] --help\" for more information about a command.\n",
    "```\n",
    "\n",
    "\n",
    "* serve：启动 ollama 服务。\n",
    "* create：根据一个 Modelfile 创建一个模型。\n",
    "* show：显示某个模型的详细信息。\n",
    "* run：运行一个模型。\n",
    "* stop：停止一个正在运行的模型。\n",
    "* pull：从一个模型仓库（registry）拉取一个模型。\n",
    "* push：将一个模型推送到一个模型仓库。\n",
    "* list：列出所有模型。\n",
    "* ps：列出所有正在运行的模型。\n",
    "* cp：复制一个模型。\n",
    "* rm：删除一个模型。\n",
    "* help：获取关于任何命令的帮助信息\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建自定义模型\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/import.md\n",
    "https://github.com/ollama/ollama/blob/main/docs/modelfile.md\n",
    "\n",
    "Create a file named Modelfile, with a FROM instruction with the local filepath to the model you want to import.\n",
    "\n",
    "```Modelfile\n",
    "FROM ./vicuna-33b.Q4_0.gguf\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "# Create the model in Ollama\n",
    "ollama create example -f Modelfile  \n",
    "\n",
    "# Run the model\n",
    "ollama run example\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize a prompt\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/modelfile.md\n",
    "\n",
    "Models from the Ollama library can be customized with a prompt.\n",
    "\n",
    "```Modelfile\n",
    "FROM llama3.2\n",
    "\n",
    "# set the temperature to 1 [higher is more creative, lower is more coherent]\n",
    "PARAMETER temperature 1\n",
    "\n",
    "# set the system message\n",
    "SYSTEM \"\"\"\n",
    "You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "\n",
    "create and run the model:\n",
    "\n",
    "```bash\n",
    "ollama create mario -f ./Modelfile\n",
    "ollama run mario\n",
    "# >>> hi\n",
    "# Hello! It's your friend Mario.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REST API\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "\n",
    "## Generate a response\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/generate -d '{\n",
    "  \"model\": \"llama3.2\",\n",
    "  \"prompt\":\"Why is the sky blue?\"\n",
    "}'\n",
    "\n",
    "# {\n",
    "#   \"model\": \"llama3.2\",\n",
    "#   \"created_at\": \"2023-08-04T08:52:19.385406455-07:00\",\n",
    "#   \"response\": \"The\",\n",
    "#   \"done\": false\n",
    "# }\n",
    "```\n",
    "\n",
    "\n",
    "## Chat with a model\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/chat -d '{\n",
    "  \"model\": \"llama3.2\",\n",
    "  \"messages\": [\n",
    "    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n",
    "  ]\n",
    "}'\n",
    "\n",
    "# {\n",
    "#   \"model\": \"llama3.2\",\n",
    "#   \"created_at\": \"2023-08-04T08:52:19.385406455-07:00\",\n",
    "#   \"message\": {\n",
    "#     \"role\": \"assistant\",\n",
    "#     \"content\": \"The\",\n",
    "#     \"images\": null\n",
    "#   },\n",
    "#   \"done\": false\n",
    "# }\n",
    "```\n",
    "\n",
    "\n",
    "## Create a Model\n",
    "\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#create-a-model\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/create -d '{\n",
    "  \"model\": \"mario\",\n",
    "  \"from\": \"llama3.2\",\n",
    "  \"system\": \"You are Mario from Super Mario Bros.\"\n",
    "}'\n",
    "```\n",
    "\n",
    "\n",
    "## List Local Models\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#list-local-models\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/tags\n",
    "```\n",
    "\n",
    "## Show Model Information\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#show-model-information\n",
    "\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/show -d '{\n",
    "  \"model\": \"llama3.2\"\n",
    "}'\n",
    "```\n",
    "\n",
    "## List Running Models\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#list-running-models\n",
    "\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/ps\n",
    "```\n",
    "\n",
    "## Generate Embedding\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embedding\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/embeddings -d '{\n",
    "  \"model\": \"all-minilm\",\n",
    "  \"prompt\": \"Here is an article about llamas...\"\n",
    "}'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export model gguf\n",
    "\n",
    "\n",
    "PyTorch 格式（.bin 或 .pt） ：原始模型权重文件，通常需要转换为 GGUF 或 GGML 格式后才能使用。\n",
    "Safetensors 格式 ：一种安全且高效的权重存储格式，常用于 Hugging Face 模型。\n",
    "不过，这些格式通常不会直接作为 Ollama 的默认模型格式，而是需要经过转换。\n",
    "\n",
    "\n",
    "Ollama 会将下载的模型存储在本地目录中。默认情况下，模型文件通常位于以下路径：\n",
    "\n",
    "Linux/macOS : ~/.ollama/models/\n",
    "\n",
    "```bash\n",
    "ollama show deepseek-r1:1.5b --modelfile | head -n 10\n",
    "\n",
    "# Modelfile generated by \"ollama show\"\n",
    "# To build a new Modelfile based on this, replace FROM with:\n",
    "# FROM deepseek-r1:1.5b\n",
    "# FROM ~/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc\n",
    "\n",
    "cd ~/.ollama/models/\n",
    "cat ~/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc > ollama-export-deepseek-r1-1.5B.gguf\n",
    "```\n",
    "\n",
    "## guuf to PyTorch 格式\n",
    "\n",
    "\n",
    "```bash\n",
    "python3 convert-pth-to-ggml.py <path_to_model> <output_path>\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
