{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ollama 介绍\n",
    "\n",
    "Get up and running with large language models.\n",
    "\n",
    "\n",
    "https://ollama.com/\n",
    "https://github.com/ollama/ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UI 交互"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载 ollama\n",
    "\n",
    "\n",
    "https://ollama.com/download\n",
    "\n",
    "可以直接下载安装包，支持 windows、Mac、Linux 三端。\n",
    "\n",
    "\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/install.sh | bash\n",
    "\n",
    "ollama --version\n",
    "# output: ollama version is 0.5.11\n",
    "```\n",
    "\n",
    "\n",
    "Docker 安装\n",
    "\n",
    "```bash\n",
    "docker pull ollama/ollama\n",
    "docker run -p 11434:11434 ollama/ollama\n",
    "```\n",
    "\n",
    "访问 http://localhost:11434 即可使用 Ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## 下载与运行模型\n",
    "\n",
    "You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.\n",
    "\n",
    "下载模型\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.2\n",
    "```\n",
    "\n",
    "\n",
    "运行模型\n",
    "\n",
    "注：如果运行模型时，模型尚未下载，则会自动下载，下载完成后自动运行模型。\n",
    "\n",
    "\n",
    "```bash\n",
    "ollama run llama3.2\n",
    "```\n",
    "\n",
    "运行模型后，就可以在命令行里一问一答的进行对话了。  \n",
    "\n",
    "\n",
    "\n",
    "```text\n",
    ">>> Send a message (/? for help)\n",
    ">>> /?\n",
    "Available Commands:\n",
    "  /set            Set session variables\n",
    "  /show           Show model information\n",
    "  /load <model>   Load a session or model\n",
    "  /save <model>   Save your current session\n",
    "  /clear          Clear session context\n",
    "  /bye            Exit\n",
    "  /?, /help       Help for a command\n",
    "  /? shortcuts    Help for keyboard shortcuts\n",
    "\n",
    "Use \"\"\" to begin a multi-line message.\n",
    "\n",
    ">>> who are you?\n",
    "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n",
    "\n",
    ">>> can you speek chinese?\n",
    "I can understand and generate text in Simplified Chinese, but my proficiency may not be as high as that of a native speaker or a professional translator.\n",
    "\n",
    "If you'd like to communicate in Chinese, I can try to:\n",
    "\n",
    "1. Understand and respond to simple questions or phrases\n",
    "2. Generate text in Simplified Chinese on various topics\n",
    "3. Translate English text into Simplified Chinese\n",
    "\n",
    "However, please note that my ability to understand nuances, idioms, and complex conversations may be limited.\n",
    "\n",
    "Which aspect of Chinese language would you like me to help with?\n",
    "```\n",
    "\n",
    "\n",
    "### Multiline input\n",
    "\n",
    "```text\n",
    ">>> \"\"\"Hello,\n",
    "... world!\n",
    "... \"\"\"\n",
    "I'm a basic program that prints the famous \"Hello, world!\" message to the console.\n",
    "```\n",
    "\n",
    "\n",
    "### Multimodal models\n",
    "\n",
    "\n",
    "```bash\n",
    "ollama run llava \"What's in this image? /Users/jmorgan/Desktop/smile.png\"\n",
    "```\n",
    "\n",
    "### Pass the prompt as an argument\n",
    "\n",
    "```bash\n",
    "ollama run llama3.2 \"Summarize this file: $(cat README.md)\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDK 交互\n",
    "\n",
    "https://github.com/ollama/ollama-python/tree/main/examples\n",
    "\n",
    "安装 python SDK\n",
    "\n",
    "\n",
    "## 安装 SDK\n",
    "\n",
    "```bash\n",
    "pip install ollama\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homePath:  /Users/tiankonguse-m3\n",
      "basePath:  /Users/tiankonguse-m3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# python 获取系统变量 HOME 变量\n",
    "import os\n",
    "homePath = os.environ['HOME']\n",
    "print(\"homePath: \",homePath)\n",
    "\n",
    "# 修改成自己的 HOME 路径\n",
    "basePath=homePath\n",
    "\n",
    "print(\"basePath: \",basePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建议手动在命令行里运行\n",
    "%pip install ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本补全 generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Who are you\n",
      "Response str: I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n",
      "\n",
      "prompt: What is your name\n",
      "Response str: I don't have a personal name. I'm an AI designed to assist and provide information, and I'm often referred to as a \"language model\" or a \"chatbot.\" My purpose is to help users like you with their questions and tasks, and I don't have a personal identity or emotions. Is there anything else I can help you with?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import shlex\n",
    "import ollama\n",
    "import json\n",
    "\n",
    "prompts = [\"Who are you\", \"What is your name\"]\n",
    "\n",
    "# Iterate over prompts and generate responses\n",
    "for prompt in prompts:\n",
    "    response = ollama.generate(\n",
    "        model=\"llama3.2\",  # 模型名称\n",
    "        prompt=prompt  # 提示文本\n",
    "    )\n",
    "\n",
    "    response = response.response\n",
    "\n",
    "    # 打印 response 的类型\n",
    "    # print(\"Response Type:\", type(response))\n",
    "\n",
    "\n",
    "    print(\"prompt:\", prompt)\n",
    "    # 如果 response 是 数组，使用 ”“ 连接数组\n",
    "    if isinstance(response, list):\n",
    "        print(\"Response list:\", \" \".join(response)) \n",
    "    elif isinstance(response, str):\n",
    "        print(\"Response str:\", response)\n",
    "    else:\n",
    "        print(\"Response other:\", response)\n",
    "    print(\"\")\n",
    "\n",
    "# prompt: Who are you\n",
    "# Response str: I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n",
    "\n",
    "# prompt: What is your name\n",
    "# Response str: I don't have a personal name, but I'm an AI designed to assist and communicate with users. You can think of me as a conversational AI or a chatbot. I'm here to help answer your questions, provide information, and engage in conversations to the best of my abilities. Is there something specific you'd like to talk about or ask?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对话模式(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "response = chat(\n",
    "    model=\"llama3.2\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Who are you?\"}\n",
    "    ]\n",
    ")\n",
    "# 转化为 json 格式化打印 response\n",
    "\n",
    "print(response.message.content)\n",
    "# I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 流式响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\""
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "stream = chat(\n",
    "    model=\"llama3.2\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Who are you?\"}],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# 效果：单词一个个出来\n",
    "for chunk in stream:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "# I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他 SDK API\n",
    "\n",
    "- 列出所有可用的模型 `ollama.list()`\n",
    "- 显示指定模型的详细信息 `ollama.show('llama3.2')`\n",
    "- 从远程仓库拉取模型 `ollama.pull('llama3.2')`\n",
    "- 生成文本嵌入 `ollama.embed(model='llama3.2', input='The sky is blue because of rayleigh scattering')`\n",
    "- 查看正在运行的模型列表 `ollama.ps()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动补全模式 fill-in-middle\n",
    "\n",
    "填充开头开结尾，自动补充中间\n",
    "\n",
    "\n",
    "使用场景：\n",
    "\n",
    "- 代码补全， 模型：codellama:7b-code\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sort the string s in ascending order \"\"\"\n",
      "    # Convert the string to a list of characters\n",
      "    char_list = list(s)\n",
      "    \n",
      "    # Sort the list of characters\n",
      "    char_list.sort()\n",
      "    \n",
      "    # Join the sorted list back into a string\n",
      "    result = ''.join(char_list)\n",
      "    \n",
      "    return result\n",
      "\n",
      "# Example usage:\n",
      "input_string = \"hello\"\n",
      "sorted_string = Sort(input_string)\n",
      "print(sorted_string)  # Output: \"ehllo\"  # The characters are sorted in ascending order\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ollama import generate\n",
    "\n",
    "prompt = '''def Sort(s: str) -> str:\n",
    "    \"\"\" '''\n",
    "\n",
    "suffix = \"\"\"\n",
    "    return result\n",
    "\"\"\"\n",
    "\n",
    "response = generate(\n",
    "  model='qwen2.5-coder:0.5b',\n",
    "  prompt=prompt,\n",
    "  suffix=suffix,\n",
    "  options={\n",
    "    'num_predict': 128,\n",
    "    'temperature': 0,\n",
    "    'top_p': 0.9,\n",
    "    'stop': ['<EOT>'],\n",
    "  },\n",
    ")\n",
    "\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## call function\n",
    "\n",
    "自定义插件函数，大模型预处理后，主动调研插件，结果再传给大模型，最后输出结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is 1 + 4 - 2 * 6?\n",
      "Calling function: add_two_numbers\n",
      "Arguments: {'a': '7', 'b': '4'}\n",
      "Function output: 11\n",
      "Calling function: subtract_two_numbers\n",
      "Arguments: {'a': '3', 'b': '-12'}\n",
      "Function output: 15\n",
      "Tool call result added to messages:  [{'role': 'user', 'content': 'What is 1 + 4 - 2 * 6?'}, Message(role='assistant', content='', images=None, tool_calls=[ToolCall(function=Function(name='add_two_numbers', arguments={'a': '7', 'b': '4'})), ToolCall(function=Function(name='subtract_two_numbers', arguments={'a': '3', 'b': '-12'}))]), {'role': 'tool', 'content': '15', 'name': 'subtract_two_numbers'}]\n",
      "Final response: To calculate the expression 1 + 4 - 2 * 6, we need to follow the order of operations (PEMDAS):\n",
      "\n",
      "1. Multiply 2 and 6: 2 * 6 = 12\n",
      "2. Add 1 and 4: 1 + 4 = 5\n",
      "3. Subtract 12 from 5: 5 - 12 = -7\n",
      "\n",
      "So, the final result is -7.\n"
     ]
    }
   ],
   "source": [
    "from ollama import ChatResponse, chat\n",
    "\n",
    "\n",
    "def add_two_numbers(a: int, b: int) -> int:\n",
    "  \"\"\"\n",
    "  Add two numbers\n",
    "\n",
    "  Args:\n",
    "    a (int): The first number\n",
    "    b (int): The second number\n",
    "\n",
    "  Returns:\n",
    "    int: The sum of the two numbers\n",
    "  \"\"\"\n",
    "\n",
    "  # The cast is necessary as returned tool call arguments don't always conform exactly to schema\n",
    "  # E.g. this would prevent \"what is 30 + 12\" to produce '3012' instead of 42\n",
    "  return int(a) + int(b)\n",
    "\n",
    "\n",
    "def subtract_two_numbers(a: int, b: int) -> int:\n",
    "  \"\"\"\n",
    "  Subtract two numbers\n",
    "  \"\"\"\n",
    "\n",
    "  # The cast is necessary as returned tool call arguments don't always conform exactly to schema\n",
    "  return int(a) - int(b)\n",
    "\n",
    "\n",
    "# Tools can still be manually defined and passed into chat\n",
    "subtract_two_numbers_tool = {\n",
    "  'type': 'function',\n",
    "  'function': {\n",
    "    'name': 'subtract_two_numbers',\n",
    "    'description': 'Subtract two numbers',\n",
    "    'parameters': {\n",
    "      'type': 'object',\n",
    "      'required': ['a', 'b'],\n",
    "      'properties': {\n",
    "        'a': {'type': 'integer', 'description': 'The first number'},\n",
    "        'b': {'type': 'integer', 'description': 'The second number'},\n",
    "      },\n",
    "    },\n",
    "  },\n",
    "}\n",
    "\n",
    "messages = [{'role': 'user', 'content': 'What is 1 + 4 - 2 * 6?'}]\n",
    "print('Prompt:', messages[0]['content'])\n",
    "\n",
    "available_functions = {\n",
    "  'add_two_numbers': add_two_numbers,\n",
    "  'subtract_two_numbers': subtract_two_numbers,\n",
    "}\n",
    "\n",
    "model = \"llama3.2\"\n",
    "\n",
    "response: ChatResponse = chat(\n",
    "  model,\n",
    "  messages=messages,\n",
    "  tools=[add_two_numbers, subtract_two_numbers_tool],\n",
    ")\n",
    "\n",
    "if response.message.tool_calls:\n",
    "  # There may be multiple tool calls in the response\n",
    "  for tool in response.message.tool_calls:\n",
    "    # Ensure the function is available, and then call it\n",
    "    if function_to_call := available_functions.get(tool.function.name):\n",
    "      print('Calling function:', tool.function.name)\n",
    "      print('Arguments:', tool.function.arguments)\n",
    "      output = function_to_call(**tool.function.arguments)\n",
    "      print('Function output:', output)\n",
    "    else:\n",
    "      print('Function', tool.function.name, 'not found')\n",
    "\n",
    "# Only needed to chat with the model using the tool call results\n",
    "if response.message.tool_calls:\n",
    "  # Add the function response to messages for the model to use\n",
    "  messages.append(response.message)\n",
    "  messages.append({'role': 'tool', 'content': str(output), 'name': tool.function.name})\n",
    "  \n",
    "  print('Tool call result added to messages: ', messages)\n",
    "  # Get final response from model with function outputs\n",
    "  final_response = chat(model, messages=messages)\n",
    "  print('Final response:', final_response.message.content)\n",
    "\n",
    "else:\n",
    "  print('No tool calls returned from model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chat with image\n",
    "\n",
    "对话时，带上图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image depicts a woman standing on a wet street, wearing an elegant white dress. Her long brown hair flows freely as she twirls her skirt and gazes directly at the camera with a warm smile. The dress features thin straps, a fitted bodice, and a flowing skirt that adds to her graceful pose. She wears open-toed heels that complement her outfit, and her right arm is raised above her head while holding onto her skirt with her left hand.\n",
      "\n",
      "The background of the image is blurred but appears to be a city street or park during the evening hours, with lights reflecting off the wet pavement and trees visible in the distance. The overall atmosphere suggests a romantic or celebratory setting, possibly a photo shoot or special occasion.\n",
      " In the image, there is a young woman standing on a rain-soaked street during what appears to be either dawn or dusk. She is holding her hair with one hand while waving at the camera with the other. The woman is dressed in a flowing white dress, which contrasts with the dark and moody ambiance of the scene. The sky is overcast, and there are street lamps illuminating the area. A car can be seen on the left side of the image, and there are trees along the sidewalk. The overall atmosphere is romantic and slightly melancholic due to the weather conditions and the solitude of the woman on the street. \n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "from ollama import chat\n",
    "\n",
    "# from pathlib import Path\n",
    "\n",
    "# Pass in the path to the image\n",
    "# path = input('Pass in the path to the image')\n",
    "path = basePath + '/project/github/ComfyUI/output/ComfyUI_00098_.png'\n",
    "# path= basePath + '/project/github/faceswap/photo/wyz_wbq/video-frame-1.png'\n",
    "\n",
    "model = 'llava' # 速度很快\n",
    "model = 'llama3.2-vision' # 速度很慢\n",
    "# model = 'llama3.2-vision:11b'\n",
    "\n",
    "response = chat(\n",
    "  model=model,\n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': 'What is in this image? More details',\n",
    "      'images': [path],\n",
    "    }\n",
    "  ],\n",
    ")\n",
    "\n",
    "print(response.message.content)\n",
    "\n",
    "\n",
    "# You can also pass in base64 encoded image data\n",
    "img = base64.b64encode(Path(path).read_bytes()).decode()\n",
    "# or the raw bytes\n",
    "# img = Path(path).read_bytes()\n",
    "\n",
    "response = chat(\n",
    "  model='llava',\n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': 'What is in this image? Can you describe it?',\n",
    "      'images': [img],\n",
    "    }\n",
    "  ],\n",
    ")\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate with image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This is a photograph of a woman walking down the street in what appears to be a city setting during rainy weather. The woman is wearing a sleeveless dress, which suggests that it might be a warmer season or the climate is such that she's dressed for comfort rather than cold. She has her left arm outstretched and her right hand lightly touching her skirt as if gently spinning around, which adds a sense of movement and joy to the scene.\n",
      "\n",
      "The rain is visible in the background, adding a serene and somewhat moody atmosphere to the image. The wet surface of the street reflects the glow of streetlights, creating a warm contrast to the cool tones of the overcast sky. The woman's pose and expression convey a feeling of carefree enjoyment, possibly on her way home or exploring the city.\n",
      "\n",
      "The overall composition of the photograph captures a moment of everyday life with an artistic touch, emphasizing the interplay between human emotion, urban architecture, and weather conditions. \n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "from ollama import generate\n",
    "\n",
    "# Pass in the path to the image\n",
    "path = basePath + '/project/github/ComfyUI/output/ComfyUI_00098_.png'\n",
    "img = base64.b64encode(Path(path).read_bytes()).decode()\n",
    "\n",
    "\n",
    "\n",
    "for response in generate('llava', 'explain this image:', images=[img], stream=True):\n",
    "  print(response['response'], end='', flush=True)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## structured-outputs-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"summary\": \"A woman is captured in mid-stride on a rainy night, with her arms outstretched as if embracing the moment. She's wearing a white dress and heels, which contrast with the dark, wet street around her.\",\n",
      "  \"objects\": [\n",
      "    {\n",
      "      \"name\": \"Woman\",\n",
      "      \"confidence\": 0.95,\n",
      "      \"attributes\": \"She is in motion, smiling, and appears to be enjoying herself despite the rain.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Street\",\n",
      "      \"confidence\": 0.85,\n",
      "      \"attributes\": \"The street is wet from recent rainfall, with puddles visible on the asphalt.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Rain\",\n",
      "      \"confidence\": 0.75,\n",
      "      \"attributes\": \"It's raining, creating a reflective sheen on the street and sidewalk.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Dress\",\n",
      "      \"confidence\": 0.85,\n",
      "      \"attributes\": \"The woman is wearing a white dress that stands out against the darker tones of the scene.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Heels\",\n",
      "      \"confidence\": 0.75,\n",
      "      \"attributes\": \"She's wearing high heels, which add an element of style to her outfit.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Street Lights\",\n",
      "      \"confidence\": 0.65,\n",
      "      \"attributes\": \"There are street lights in the background that provide a warm glow despite the rain.\"\n",
      "    }\n",
      "  ],\n",
      "  \"scene\": \"The scene is set on a city street at night during a rain shower.\",\n",
      "  \"colors\": [\n",
      "    \"White\",\n",
      "    \"Black\",\n",
      "    \"Gray\",\n",
      "    \"Red\",\n",
      "    \"Yellow\"\n",
      "  ],\n",
      "  \"time_of_day\": \"Night\",\n",
      "  \"setting\": \"Unknown\",\n",
      "  \"text_content\": \"There is no visible text in the image.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from ollama import chat\n",
    "\n",
    "\n",
    "# Define the schema for image objects\n",
    "class Object(BaseModel):\n",
    "  name: str\n",
    "  confidence: float\n",
    "  attributes: str\n",
    "\n",
    "\n",
    "class ImageDescription(BaseModel):\n",
    "  summary: str\n",
    "  objects: list[Object]\n",
    "  scene: str\n",
    "  colors: list[str]\n",
    "  time_of_day: Literal['Morning', 'Afternoon', 'Evening', 'Night']\n",
    "  setting: Literal['Indoor', 'Outdoor', 'Unknown']\n",
    "  text_content: str | None = None\n",
    "\n",
    "\n",
    "# Get path from user input\n",
    "path = basePath + '/project/github/ComfyUI/output/ComfyUI_00098_.png'\n",
    "path = Path(path)\n",
    "\n",
    "# Verify the file exists\n",
    "if not path.exists():\n",
    "  raise FileNotFoundError(f'Image not found at: {path}')\n",
    "\n",
    "model = 'llava'\n",
    "\n",
    "# Set up chat as usual\n",
    "response = chat(\n",
    "  model=model,\n",
    "  format=ImageDescription.model_json_schema(),  # Pass in the schema for the response\n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': 'Analyze this image and return a detailed JSON description including objects, scene, colors and any text detected. If you cannot determine certain details, leave those fields empty.',\n",
    "      'images': [path],\n",
    "    },\n",
    "  ],\n",
    "  options={'temperature': 0},  # Set temperature to 0 for more deterministic output\n",
    ")\n",
    "\n",
    "\n",
    "# Convert received content to the schema\n",
    "image_analysis = ImageDescription.model_validate_json(response.message.content)\n",
    "\n",
    "# json 格式化输出, 分隔符 2个空格\n",
    "json_output = image_analysis.model_dump_json(indent=2)\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama 相关命令\n",
    "\n",
    "\n",
    "## ollama --help\n",
    "\n",
    "```bash\n",
    "ollama --help\n",
    "\n",
    "```\n",
    "\n",
    "```text\n",
    "Large language model runner\n",
    "\n",
    "Usage:\n",
    "  ollama [flags]\n",
    "  ollama [command]\n",
    "\n",
    "Available Commands:\n",
    "  serve       Start ollama\n",
    "  create      Create a model from a Modelfile\n",
    "  show        Show information for a model\n",
    "  run         Run a model\n",
    "  stop        Stop a running model\n",
    "  pull        Pull a model from a registry\n",
    "  push        Push a model to a registry\n",
    "  list        List models\n",
    "  ps          List running models\n",
    "  cp          Copy a model\n",
    "  rm          Remove a model\n",
    "  help        Help about any command\n",
    "\n",
    "Flags:\n",
    "  -h, --help      help for ollama\n",
    "  -v, --version   Show version information\n",
    "\n",
    "Use \"ollama [command] --help\" for more information about a command.\n",
    "```\n",
    "\n",
    "\n",
    "* serve：启动 ollama 服务。\n",
    "* create：根据一个 Modelfile 创建一个模型。\n",
    "* show：显示某个模型的详细信息。\n",
    "* run：运行一个模型。\n",
    "* stop：停止一个正在运行的模型。\n",
    "* pull：从一个模型仓库（registry）拉取一个模型。\n",
    "* push：将一个模型推送到一个模型仓库。\n",
    "* list：列出所有模型。\n",
    "* ps：列出所有正在运行的模型。\n",
    "* cp：复制一个模型。\n",
    "* rm：删除一个模型。\n",
    "* help：获取关于任何命令的帮助信息\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建自定义模型\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/import.md\n",
    "https://github.com/ollama/ollama/blob/main/docs/modelfile.md\n",
    "\n",
    "Create a file named Modelfile, with a FROM instruction with the local filepath to the model you want to import.\n",
    "\n",
    "```Modelfile\n",
    "FROM ./vicuna-33b.Q4_0.gguf\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "# Create the model in Ollama\n",
    "ollama create example -f Modelfile  \n",
    "\n",
    "# Run the model\n",
    "ollama run example\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize a prompt\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/modelfile.md\n",
    "\n",
    "Models from the Ollama library can be customized with a prompt.\n",
    "\n",
    "```Modelfile\n",
    "FROM llama3.2\n",
    "\n",
    "# set the temperature to 1 [higher is more creative, lower is more coherent]\n",
    "PARAMETER temperature 1\n",
    "\n",
    "# set the system message\n",
    "SYSTEM \"\"\"\n",
    "You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "\n",
    "create and run the model:\n",
    "\n",
    "```bash\n",
    "ollama create mario -f ./Modelfile\n",
    "ollama run mario\n",
    "# >>> hi\n",
    "# Hello! It's your friend Mario.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REST API\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "\n",
    "## Generate a response\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/generate -d '{\n",
    "  \"model\": \"llama3.2\",\n",
    "  \"prompt\":\"Why is the sky blue?\"\n",
    "}'\n",
    "\n",
    "# {\n",
    "#   \"model\": \"llama3.2\",\n",
    "#   \"created_at\": \"2023-08-04T08:52:19.385406455-07:00\",\n",
    "#   \"response\": \"The\",\n",
    "#   \"done\": false\n",
    "# }\n",
    "```\n",
    "\n",
    "\n",
    "## Chat with a model\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/chat -d '{\n",
    "  \"model\": \"llama3.2\",\n",
    "  \"messages\": [\n",
    "    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n",
    "  ]\n",
    "}'\n",
    "\n",
    "# {\n",
    "#   \"model\": \"llama3.2\",\n",
    "#   \"created_at\": \"2023-08-04T08:52:19.385406455-07:00\",\n",
    "#   \"message\": {\n",
    "#     \"role\": \"assistant\",\n",
    "#     \"content\": \"The\",\n",
    "#     \"images\": null\n",
    "#   },\n",
    "#   \"done\": false\n",
    "# }\n",
    "```\n",
    "\n",
    "\n",
    "## Create a Model\n",
    "\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#create-a-model\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/create -d '{\n",
    "  \"model\": \"mario\",\n",
    "  \"from\": \"llama3.2\",\n",
    "  \"system\": \"You are Mario from Super Mario Bros.\"\n",
    "}'\n",
    "```\n",
    "\n",
    "\n",
    "## List Local Models\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#list-local-models\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/tags\n",
    "```\n",
    "\n",
    "## Show Model Information\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#show-model-information\n",
    "\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/show -d '{\n",
    "  \"model\": \"llama3.2\"\n",
    "}'\n",
    "```\n",
    "\n",
    "## List Running Models\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#list-running-models\n",
    "\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/ps\n",
    "```\n",
    "\n",
    "## Generate Embedding\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embedding\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/embeddings -d '{\n",
    "  \"model\": \"all-minilm\",\n",
    "  \"prompt\": \"Here is an article about llamas...\"\n",
    "}'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export model gguf\n",
    "\n",
    "\n",
    "PyTorch 格式（.bin 或 .pt） ：原始模型权重文件，通常需要转换为 GGUF 或 GGML 格式后才能使用。  \n",
    "Safetensors 格式 ：一种安全且高效的权重存储格式，常用于 Hugging Face 模型。  \n",
    "不过，这些格式通常不会直接作为 Ollama 的默认模型格式，而是需要经过转换。  \n",
    "\n",
    "\n",
    "Ollama 会将下载的模型存储在本地目录中。默认情况下，模型文件通常位于以下路径：\n",
    "\n",
    "Linux/macOS : ~/.ollama/models/\n",
    "\n",
    "```bash\n",
    "ollama show deepseek-r1:1.5b --modelfile | head -n 10\n",
    "\n",
    "# Modelfile generated by \"ollama show\"\n",
    "# To build a new Modelfile based on this, replace FROM with:\n",
    "# FROM deepseek-r1:1.5b\n",
    "# FROM ~/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc\n",
    "\n",
    "cd ~/.ollama/models/\n",
    "cat ~/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc > ollama-export-deepseek-r1-1.5B.gguf\n",
    "```\n",
    "\n",
    "## guuf to PyTorch 格式\n",
    "\n",
    "\n",
    "```bash\n",
    "python3 convert-pth-to-ggml.py <path_to_model> <output_path>\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
