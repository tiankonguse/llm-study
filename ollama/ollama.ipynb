{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ollama ä»‹ç»\n",
    "\n",
    "Get up and running with large language models.\n",
    "\n",
    "\n",
    "https://ollama.com/\n",
    "https://github.com/ollama/ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UI äº¤äº’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸‹è½½ ollama\n",
    "\n",
    "\n",
    "https://ollama.com/download\n",
    "\n",
    "å¯ä»¥ç›´æ¥ä¸‹è½½å®‰è£…åŒ…ï¼Œæ”¯æŒ windowsã€Macã€Linux ä¸‰ç«¯ã€‚\n",
    "\n",
    "\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/install.sh | bash\n",
    "\n",
    "ollama --version\n",
    "# output: ollama version is 0.5.11\n",
    "```\n",
    "\n",
    "\n",
    "Docker å®‰è£…\n",
    "\n",
    "```bash\n",
    "docker pull ollama/ollama\n",
    "docker run -p 11434:11434 ollama/ollama\n",
    "```\n",
    "\n",
    "è®¿é—® http://localhost:11434 å³å¯ä½¿ç”¨ Ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## ä¸‹è½½ä¸è¿è¡Œæ¨¡å‹\n",
    "\n",
    "You should have at least 8 GB of RAM available to run the 7B models, 16 GB to run the 13B models, and 32 GB to run the 33B models.\n",
    "\n",
    "ä¸‹è½½æ¨¡å‹\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.2\n",
    "```\n",
    "\n",
    "\n",
    "è¿è¡Œæ¨¡å‹\n",
    "\n",
    "æ³¨ï¼šå¦‚æœè¿è¡Œæ¨¡å‹æ—¶ï¼Œæ¨¡å‹å°šæœªä¸‹è½½ï¼Œåˆ™ä¼šè‡ªåŠ¨ä¸‹è½½ï¼Œä¸‹è½½å®Œæˆåè‡ªåŠ¨è¿è¡Œæ¨¡å‹ã€‚\n",
    "\n",
    "\n",
    "```bash\n",
    "ollama run llama3.2\n",
    "```\n",
    "\n",
    "è¿è¡Œæ¨¡å‹åï¼Œå°±å¯ä»¥åœ¨å‘½ä»¤è¡Œé‡Œä¸€é—®ä¸€ç­”çš„è¿›è¡Œå¯¹è¯äº†ã€‚  \n",
    "\n",
    "\n",
    "\n",
    "```text\n",
    ">>> Send a message (/? for help)\n",
    ">>> /?\n",
    "Available Commands:\n",
    "  /set            Set session variables\n",
    "  /show           Show model information\n",
    "  /load <model>   Load a session or model\n",
    "  /save <model>   Save your current session\n",
    "  /clear          Clear session context\n",
    "  /bye            Exit\n",
    "  /?, /help       Help for a command\n",
    "  /? shortcuts    Help for keyboard shortcuts\n",
    "\n",
    "Use \"\"\" to begin a multi-line message.\n",
    "\n",
    ">>> who are you?\n",
    "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n",
    "\n",
    ">>> can you speek chinese?\n",
    "I can understand and generate text in Simplified Chinese, but my proficiency may not be as high as that of a native speaker or a professional translator.\n",
    "\n",
    "If you'd like to communicate in Chinese, I can try to:\n",
    "\n",
    "1. Understand and respond to simple questions or phrases\n",
    "2. Generate text in Simplified Chinese on various topics\n",
    "3. Translate English text into Simplified Chinese\n",
    "\n",
    "However, please note that my ability to understand nuances, idioms, and complex conversations may be limited.\n",
    "\n",
    "Which aspect of Chinese language would you like me to help with?\n",
    "```\n",
    "\n",
    "\n",
    "### Multiline input\n",
    "\n",
    "```text\n",
    ">>> \"\"\"Hello,\n",
    "... world!\n",
    "... \"\"\"\n",
    "I'm a basic program that prints the famous \"Hello, world!\" message to the console.\n",
    "```\n",
    "\n",
    "\n",
    "### Multimodal models\n",
    "\n",
    "\n",
    "```bash\n",
    "ollama run llava \"What's in this image? /Users/jmorgan/Desktop/smile.png\"\n",
    "```\n",
    "\n",
    "### Pass the prompt as an argument\n",
    "\n",
    "```bash\n",
    "ollama run llama3.2 \"Summarize this file: $(cat README.md)\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDK äº¤äº’\n",
    "\n",
    "https://github.com/ollama/ollama-python/tree/main/examples\n",
    "\n",
    "å®‰è£… python SDK\n",
    "\n",
    "\n",
    "## å®‰è£… SDK\n",
    "\n",
    "```bash\n",
    "pip install ollama\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homePath:  /Users/tiankonguse-m3\n",
      "basePath:  /Users/tiankonguse-m3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# python è·å–ç³»ç»Ÿå˜é‡ HOME å˜é‡\n",
    "import os\n",
    "homePath = os.environ['HOME']\n",
    "print(\"homePath: \",homePath)\n",
    "\n",
    "# ä¿®æ”¹æˆè‡ªå·±çš„ HOME è·¯å¾„\n",
    "basePath=homePath\n",
    "\n",
    "print(\"basePath: \",basePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºè®®æ‰‹åŠ¨åœ¨å‘½ä»¤è¡Œé‡Œè¿è¡Œ\n",
    "%pip install ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ–‡æœ¬è¡¥å…¨ generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: Who are you\n",
      "Response str: I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n",
      "\n",
      "prompt: What is your name\n",
      "Response str: I don't have a personal name, but I'm an AI designed to assist and communicate with users. You can think of me as a helpful computer program or a conversational robot. Some people might refer to me as \"Assistant\" or \"AI Assistant,\" but I don't have a specific name. How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import shlex\n",
    "import ollama\n",
    "import json\n",
    "\n",
    "prompts = [\"Who are you\", \"What is your name\"]\n",
    "\n",
    "# Iterate over prompts and generate responses\n",
    "for prompt in prompts:\n",
    "    response = ollama.generate(\n",
    "        model=\"llama3.2\",  # æ¨¡å‹åç§°\n",
    "        prompt=prompt  # æç¤ºæ–‡æœ¬\n",
    "    )\n",
    "\n",
    "    response = response.response\n",
    "\n",
    "    # æ‰“å° response çš„ç±»å‹\n",
    "    # print(\"Response Type:\", type(response))\n",
    "\n",
    "\n",
    "    print(\"prompt:\", prompt)\n",
    "    # å¦‚æœ response æ˜¯ æ•°ç»„ï¼Œä½¿ç”¨ â€â€œ è¿æ¥æ•°ç»„\n",
    "    if isinstance(response, list):\n",
    "        print(\"Response list:\", \" \".join(response)) \n",
    "    elif isinstance(response, str):\n",
    "        print(\"Response str:\", response)\n",
    "    else:\n",
    "        print(\"Response other:\", response)\n",
    "    print(\"\")\n",
    "\n",
    "# prompt: Who are you\n",
    "# Response str: I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n",
    "\n",
    "# prompt: What is your name\n",
    "# Response str: I don't have a personal name, but I'm an AI designed to assist and communicate with users. You can think of me as a conversational AI or a chatbot. I'm here to help answer your questions, provide information, and engage in conversations to the best of my abilities. Is there something specific you'd like to talk about or ask?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: What is your name\n",
      "llama3.2 Response str: I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n",
      "\n",
      "prompt: What is your name\n",
      "deepseek-r1:8b Response str: <think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Greetings! I'm DeepSeek-R1, an artificial intelligence assistant created by DeepSeek. I'm at your service and would be delighted to assist you with any inquiries or tasks you may have.\n",
      "\n",
      "prompt: What is your name\n",
      "llava:latest Response str:  I am a language model designed to assist users with various tasks related to language processing. \n",
      "\n",
      "prompt: What is your name\n",
      "gemma2:2b Response str: I am Gemma, an AI assistant created by the Gemma team. I'm a large language model that can communicate and generate human-like text in response to your prompts and questions.  \n",
      "\n",
      "Think of me as a friendly chatbot! What would you like to talk about today? ğŸ˜„ \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import shlex\n",
    "import ollama\n",
    "import json\n",
    "\n",
    "models = [\"llama3.2\", \"deepseek-r1:8b\", \"llava:latest\", \"gemma2:2b\"]\n",
    "\n",
    "# Iterate over prompts and generate responses\n",
    "for model in models:\n",
    "    response = ollama.generate(\n",
    "        model=model,  # æ¨¡å‹åç§°\n",
    "        prompt=\"Who are you\"  # æç¤ºæ–‡æœ¬\n",
    "    )\n",
    "\n",
    "    response = response.response\n",
    "\n",
    "    # æ‰“å° response çš„ç±»å‹\n",
    "    # print(\"Response Type:\", type(response))\n",
    "\n",
    "\n",
    "    print(\"prompt:\", prompt)\n",
    "    # å¦‚æœ response æ˜¯ æ•°ç»„ï¼Œä½¿ç”¨ â€â€œ è¿æ¥æ•°ç»„\n",
    "    if isinstance(response, list):\n",
    "        print(model, \"Response list:\", \" \".join(response)) \n",
    "    elif isinstance(response, str):\n",
    "        print(model, \"Response str:\", response)\n",
    "    else:\n",
    "        print(model, \"Response other:\", response)\n",
    "    print(\"\")\n",
    "\n",
    "# prompt: Who are you\n",
    "# Response str: I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n",
    "\n",
    "# prompt: What is your name\n",
    "# Response str: I don't have a personal name, but I'm an AI designed to assist and communicate with users. You can think of me as a conversational AI or a chatbot. I'm here to help answer your questions, provide information, and engage in conversations to the best of my abilities. Is there something specific you'd like to talk about or ask?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¯¹è¯æ¨¡å¼(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "response = chat(\n",
    "    model=\"llama3.2\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Who are you?\"}\n",
    "    ]\n",
    ")\n",
    "# è½¬åŒ–ä¸º json æ ¼å¼åŒ–æ‰“å° response\n",
    "\n",
    "print(response.message.content)\n",
    "# I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æµå¼å“åº”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\""
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "stream = chat(\n",
    "    model=\"llama3.2\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Who are you?\"}],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# æ•ˆæœï¼šå•è¯ä¸€ä¸ªä¸ªå‡ºæ¥\n",
    "for chunk in stream:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "# I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å…¶ä»– SDK API\n",
    "\n",
    "- åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹ `ollama.list()`\n",
    "- æ˜¾ç¤ºæŒ‡å®šæ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ `ollama.show('llama3.2')`\n",
    "- ä»è¿œç¨‹ä»“åº“æ‹‰å–æ¨¡å‹ `ollama.pull('llama3.2')`\n",
    "- ç”Ÿæˆæ–‡æœ¬åµŒå…¥ `ollama.embed(model='llama3.2', input='The sky is blue because of rayleigh scattering')`\n",
    "- æŸ¥çœ‹æ­£åœ¨è¿è¡Œçš„æ¨¡å‹åˆ—è¡¨ `ollama.ps()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è‡ªåŠ¨è¡¥å…¨æ¨¡å¼ fill-in-middle\n",
    "\n",
    "å¡«å……å¼€å¤´å¼€ç»“å°¾ï¼Œè‡ªåŠ¨è¡¥å……ä¸­é—´\n",
    "\n",
    "\n",
    "ä½¿ç”¨åœºæ™¯ï¼š\n",
    "\n",
    "- ä»£ç è¡¥å…¨ï¼Œ æ¨¡å‹ï¼šcodellama:7b-code\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sort the string s in ascending order \"\"\"\n",
      "    # Convert the string to a list of characters\n",
      "    char_list = list(s)\n",
      "    \n",
      "    # Sort the list of characters\n",
      "    char_list.sort()\n",
      "    \n",
      "    # Join the sorted list back into a string\n",
      "    result = ''.join(char_list)\n",
      "    \n",
      "    return result\n",
      "\n",
      "# Example usage:\n",
      "input_string = \"hello\"\n",
      "sorted_string = Sort(input_string)\n",
      "print(sorted_string)  # Output: \"ehllo\"  # The characters are sorted in ascending order\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ollama import generate\n",
    "\n",
    "prompt = '''def Sort(s: str) -> str:\n",
    "    \"\"\" '''\n",
    "\n",
    "suffix = \"\"\"\n",
    "    return result\n",
    "\"\"\"\n",
    "\n",
    "response = generate(\n",
    "  model='qwen2.5-coder:0.5b',\n",
    "  prompt=prompt,\n",
    "  suffix=suffix,\n",
    "  options={\n",
    "    'num_predict': 128,\n",
    "    'temperature': 0,\n",
    "    'top_p': 0.9,\n",
    "    'stop': ['<EOT>'],\n",
    "  },\n",
    ")\n",
    "\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## call function\n",
    "\n",
    "è‡ªå®šä¹‰æ’ä»¶å‡½æ•°ï¼Œå¤§æ¨¡å‹é¢„å¤„ç†åï¼Œä¸»åŠ¨è°ƒç ”æ’ä»¶ï¼Œç»“æœå†ä¼ ç»™å¤§æ¨¡å‹ï¼Œæœ€åè¾“å‡ºç»“æœã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is 1 + 4 - 2 * 6?\n",
      "Calling function: add_two_numbers\n",
      "Arguments: {'a': '7', 'b': '4'}\n",
      "Function output: 11\n",
      "Calling function: subtract_two_numbers\n",
      "Arguments: {'a': '3', 'b': '-12'}\n",
      "Function output: 15\n",
      "Tool call result added to messages:  [{'role': 'user', 'content': 'What is 1 + 4 - 2 * 6?'}, Message(role='assistant', content='', images=None, tool_calls=[ToolCall(function=Function(name='add_two_numbers', arguments={'a': '7', 'b': '4'})), ToolCall(function=Function(name='subtract_two_numbers', arguments={'a': '3', 'b': '-12'}))]), {'role': 'tool', 'content': '15', 'name': 'subtract_two_numbers'}]\n",
      "Final response: To calculate the expression 1 + 4 - 2 * 6, we need to follow the order of operations (PEMDAS):\n",
      "\n",
      "1. Multiply 2 and 6: 2 * 6 = 12\n",
      "2. Add 1 and 4: 1 + 4 = 5\n",
      "3. Subtract 12 from 5: 5 - 12 = -7\n",
      "\n",
      "So, the final result is -7.\n"
     ]
    }
   ],
   "source": [
    "from ollama import ChatResponse, chat\n",
    "\n",
    "\n",
    "def add_two_numbers(a: int, b: int) -> int:\n",
    "  \"\"\"\n",
    "  Add two numbers\n",
    "\n",
    "  Args:\n",
    "    a (int): The first number\n",
    "    b (int): The second number\n",
    "\n",
    "  Returns:\n",
    "    int: The sum of the two numbers\n",
    "  \"\"\"\n",
    "\n",
    "  # The cast is necessary as returned tool call arguments don't always conform exactly to schema\n",
    "  # E.g. this would prevent \"what is 30 + 12\" to produce '3012' instead of 42\n",
    "  return int(a) + int(b)\n",
    "\n",
    "\n",
    "def subtract_two_numbers(a: int, b: int) -> int:\n",
    "  \"\"\"\n",
    "  Subtract two numbers\n",
    "  \"\"\"\n",
    "\n",
    "  # The cast is necessary as returned tool call arguments don't always conform exactly to schema\n",
    "  return int(a) - int(b)\n",
    "\n",
    "\n",
    "# Tools can still be manually defined and passed into chat\n",
    "subtract_two_numbers_tool = {\n",
    "  'type': 'function',\n",
    "  'function': {\n",
    "    'name': 'subtract_two_numbers',\n",
    "    'description': 'Subtract two numbers',\n",
    "    'parameters': {\n",
    "      'type': 'object',\n",
    "      'required': ['a', 'b'],\n",
    "      'properties': {\n",
    "        'a': {'type': 'integer', 'description': 'The first number'},\n",
    "        'b': {'type': 'integer', 'description': 'The second number'},\n",
    "      },\n",
    "    },\n",
    "  },\n",
    "}\n",
    "\n",
    "messages = [{'role': 'user', 'content': 'What is 1 + 4 - 2 * 6?'}]\n",
    "print('Prompt:', messages[0]['content'])\n",
    "\n",
    "available_functions = {\n",
    "  'add_two_numbers': add_two_numbers,\n",
    "  'subtract_two_numbers': subtract_two_numbers,\n",
    "}\n",
    "\n",
    "model = \"llama3.2\"\n",
    "\n",
    "response: ChatResponse = chat(\n",
    "  model,\n",
    "  messages=messages,\n",
    "  tools=[add_two_numbers, subtract_two_numbers_tool],\n",
    ")\n",
    "\n",
    "if response.message.tool_calls:\n",
    "  # There may be multiple tool calls in the response\n",
    "  for tool in response.message.tool_calls:\n",
    "    # Ensure the function is available, and then call it\n",
    "    if function_to_call := available_functions.get(tool.function.name):\n",
    "      print('Calling function:', tool.function.name)\n",
    "      print('Arguments:', tool.function.arguments)\n",
    "      output = function_to_call(**tool.function.arguments)\n",
    "      print('Function output:', output)\n",
    "    else:\n",
    "      print('Function', tool.function.name, 'not found')\n",
    "\n",
    "# Only needed to chat with the model using the tool call results\n",
    "if response.message.tool_calls:\n",
    "  # Add the function response to messages for the model to use\n",
    "  messages.append(response.message)\n",
    "  messages.append({'role': 'tool', 'content': str(output), 'name': tool.function.name})\n",
    "  \n",
    "  print('Tool call result added to messages: ', messages)\n",
    "  # Get final response from model with function outputs\n",
    "  final_response = chat(model, messages=messages)\n",
    "  print('Final response:', final_response.message.content)\n",
    "\n",
    "else:\n",
    "  print('No tool calls returned from model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chat with image\n",
    "\n",
    "å¯¹è¯æ—¶ï¼Œå¸¦ä¸Šå›¾ç‰‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " è¿™å¼ å›¾ç‰‡æ˜¾ç¤ºäº†ä¸€ä¸ªå¥³å­åœ¨è¡—ä¸Šæ¼‚äº®åœ°èµ°ç€ï¼Œå¥¹çš„èº«ä½“æœå·¦è€Œå³è„šå‰æ–¹ã€‚å¥¹ç©¿ç€ä¸€ä»¶ç™½è‰²çš„é•¿è¢å’Œé«˜è·Ÿé‹ï¼Œå¹¶ä¸”æ­£åœ¨ç”¨ä¸€åªæ‰‹åšå‡ºâ€œå†è§â€çš„å§¿åŠ¿æ¥å‘å‰çœ‹ã€‚èƒŒæ™¯ä¸­æœ‰ä¸€ä¸ªè¡—é“ä¸Šçš„ç¯å…‰ç…§äº®äº†ç¯å¢ƒï¼ŒåŠ ä¸Šé›¨æ°´ï¼Œåˆ›é€ äº†ä¸€ä¸ªå¤œé—´çš„æ°›å›´ã€‚å¥³å­ç¬‘ç€ï¼Œå¥¹çš„è¡¨æƒ…éå¸¸å¼€å¿ƒå’Œé«˜å…´ã€‚ \n",
      " è¿™å¼ å›¾ç‰‡æ˜¾ç¤ºäº†ä¸€ä¸ªå¥³æ€§åœ¨é›¨å¤©èµ°åœ¨è¡—ä¸Šã€‚å¥¹çš„æœè£…å’Œè‚©çš„é´å­è¡¨æ˜å¥¹æ­£åœ¨æ™šä¸Šå‡ºé—¨æˆ–å›å®¶. å¥³æ€§çš„èº«ä½“æ˜¯è‡ªç”±åœ°è½¬åŠ¨ï¼Œè¡¨ç°å‡ºå¯¹å¤œçš„çˆ±å¥½æˆ–è€…å¯¹ç”Ÿæ´»çš„ä¹è§‚æ€åº¦ã€‚åŒæ—¶ï¼Œå¥¹çš„å¤´éƒ¨æ˜¯å¾®ç¬‘ç€çš„ï¼Œæ˜¾ç¤ºå‡ºèˆ’é€‚å’Œå†…å¿ƒæ»¡è¶³çš„çŠ¶æ€ã€‚èƒŒæ™¯ä¸­çš„è¡—ç¯ç…§äº®äº†å¥¹çš„èº«ä½“å’Œå‘¨å›´çš„ç¯å¢ƒï¼Œå¢æ·»äº†ä¸€äº›å…‰å’Œè‰²å½©ã€‚è¿™ä¸ªåœºæ™¯ç»™äººä»¬ç•™ä¸‹äº†ä¸€ç§æ”¾æ¾ã€å¿«ä¹çš„å¤œæ™šç”Ÿæ´»æ„Ÿè§‰ï¼Œè®©äººä»¬æƒ³è¦åœ¨é›¨å¤©å‡ºé—¨æˆ–è€…å›å®¶äº«å—è¿™ç§ç¾å¥½çš„æ—¶åˆ»ã€‚ \n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "from ollama import chat\n",
    "\n",
    "# from pathlib import Path\n",
    "\n",
    "# Pass in the path to the image\n",
    "# path = input('Pass in the path to the image')\n",
    "path = basePath + '/project/github/ComfyUI/output/ComfyUI_00098_.png'\n",
    "# path= basePath + '/project/github/faceswap/photo/wyz_wbq/video-frame-1.png'\n",
    "\n",
    "model = 'llava' # é€Ÿåº¦å¾ˆå¿«\n",
    "# model = 'llama3.2-vision' # é€Ÿåº¦å¾ˆæ…¢\n",
    "# model = 'llama3.2-vision:11b'\n",
    "\n",
    "response = chat(\n",
    "  model=model,\n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': 'è¿™å¼ å›¾ç‰‡é‡Œæ˜¯ä»€ä¹ˆï¼Ÿä½ èƒ½ä½¿ç”¨ä¸­æ–‡æè¿°ä¸€ä¸‹å—ï¼Ÿ',\n",
    "      'images': [path],\n",
    "    }\n",
    "  ],\n",
    ")\n",
    "\n",
    "print(response.message.content)\n",
    "# import base64\n",
    "from pathlib import Path\n",
    "from ollama import chat\n",
    "\n",
    "# from pathlib import Path\n",
    "\n",
    "# Pass in the path to the image\n",
    "# path = input('Pass in the path to the image')\n",
    "path = basePath + '/project/github/ComfyUI/output/ComfyUI_00098_.png'\n",
    "# path= basePath + '/project/github/faceswap/photo/wyz_wbq/video-frame-1.png'\n",
    "\n",
    "model = 'llava' # é€Ÿåº¦å¾ˆå¿«\n",
    "# model = 'llama3.2-vision' # é€Ÿåº¦å¾ˆæ…¢\n",
    "# model = 'llama3.2-vision:11b'\n",
    "\n",
    "response = chat(\n",
    "  model=model,\n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': 'è¿™å¼ å›¾ç‰‡é‡Œæ˜¯ä»€ä¹ˆï¼Ÿä½ èƒ½ä½¿ç”¨ä¸­æ–‡æè¿°ä¸€ä¸‹å—ï¼Ÿ',\n",
    "      'images': [path],\n",
    "    }\n",
    "  ],\n",
    ")\n",
    "\n",
    "print(response.message.content)\n",
    "\n",
    "\n",
    "# You can also pass in base64 encoded image data\n",
    "img = base64.b64encode(Path(path).read_bytes()).decode()\n",
    "# or the raw bytes\n",
    "# img = Path(path).read_bytes()\n",
    "\n",
    "response = chat(\n",
    "  model='llava',\n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': 'è¿™å¼ å›¾ç‰‡é‡Œæ˜¯ä»€ä¹ˆï¼Ÿä½ èƒ½ä½¿ç”¨ä¸­æ–‡æè¿°ä¸€ä¸‹å—ï¼Ÿ',\n",
    "      'images': [img],\n",
    "    }\n",
    "  ],\n",
    ")\n",
    "\n",
    "print(response.message.content)\n",
    "#   è¿™å¼ å›¾ç‰‡æ˜¾ç¤ºäº†ä¸€ä¸ªå¥³å­åœ¨è¡—ä¸Šæ¼‚äº®åœ°èµ°ç€ï¼Œå¥¹çš„èº«ä½“æœå·¦è€Œå³è„šå‰æ–¹ã€‚å¥¹ç©¿ç€ä¸€ä»¶ç™½è‰²çš„é•¿è¢å’Œé«˜è·Ÿé‹ï¼Œå¹¶ä¸”æ­£åœ¨ç”¨ä¸€åªæ‰‹åšå‡ºâ€œå†è§â€çš„å§¿åŠ¿æ¥å‘å‰çœ‹ã€‚èƒŒæ™¯ä¸­æœ‰ä¸€ä¸ªè¡—é“ä¸Šçš„ç¯å…‰ç…§äº®äº†ç¯å¢ƒï¼ŒåŠ ä¸Šé›¨æ°´ï¼Œåˆ›é€ äº†ä¸€ä¸ªå¤œé—´çš„æ°›å›´ã€‚å¥³å­ç¬‘ç€ï¼Œå¥¹çš„è¡¨æƒ…éå¸¸å¼€å¿ƒå’Œé«˜å…´ \n",
    "\n",
    "# You can also pass in base64 encoded image data\n",
    "img = base64.b64encode(Path(path).read_bytes()).decode()\n",
    "# or the raw bytes\n",
    "# img = Path(path).read_bytes()\n",
    "\n",
    "response = chat(\n",
    "  model='llava',\n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': 'è¿™å¼ å›¾ç‰‡é‡Œæ˜¯ä»€ä¹ˆï¼Ÿä½ èƒ½ä½¿ç”¨ä¸­æ–‡æè¿°ä¸€ä¸‹å—ï¼Ÿ',\n",
    "      'images': [img],\n",
    "    }\n",
    "  ],\n",
    ")\n",
    "\n",
    "print(response.message.content)\n",
    "#  è¿™å¼ å›¾ç‰‡æ˜¾ç¤ºäº†ä¸€ä¸ªå¥³æ€§åœ¨é›¨å¤©èµ°åœ¨è¡—ä¸Šã€‚å¥¹çš„æœè£…å’Œè‚©çš„é´å­è¡¨æ˜å¥¹æ­£åœ¨æ™šä¸Šå‡ºé—¨æˆ–å›å®¶. å¥³æ€§çš„èº«ä½“æ˜¯è‡ªç”±åœ°è½¬åŠ¨ï¼Œè¡¨ç°å‡ºå¯¹å¤œçš„çˆ±å¥½æˆ–è€…å¯¹ç”Ÿæ´»çš„ä¹è§‚æ€åº¦ã€‚åŒæ—¶ï¼Œå¥¹çš„å¤´éƒ¨æ˜¯å¾®ç¬‘ç€çš„ï¼Œæ˜¾ç¤ºå‡ºèˆ’é€‚å’Œå†…å¿ƒæ»¡è¶³çš„çŠ¶æ€ã€‚èƒŒæ™¯ä¸­çš„è¡—ç¯ç…§äº®äº†å¥¹çš„èº«ä½“å’Œå‘¨å›´çš„ç¯å¢ƒï¼Œå¢æ·»äº†ä¸€äº›å…‰å’Œè‰²å½©ã€‚è¿™ä¸ªåœºæ™¯ç»™äººä»¬ç•™ä¸‹äº†ä¸€ç§æ”¾æ¾ã€å¿«ä¹çš„å¤œæ™šç”Ÿæ´»æ„Ÿè§‰ï¼Œè®©äººä»¬æƒ³è¦åœ¨é›¨å¤©å‡ºé—¨æˆ–è€…å›å®¶äº«å—è¿™ç§ç¾å¥½çš„æ—¶åˆ»ã€‚ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate with image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " è¿™å¼ ç…§ç‰‡æ˜¯åœ¨å‚²é›¨å¤©æ°”çš„è¡—é“ä¸Šæ‹æ‘„çš„ã€‚å¥³å­æ­£åœ¨æ­¥è¡Œï¼Œå¥¹çš„æŠ«è¡£ä¸ºè–„è‰²çš„çº±æœè£…ï¼Œä½©ç€ä¸€åŒé«˜è·Ÿé‹ã€‚å¥¹æ‰‹ä¸Šæ¡ç€ä¸€ä¸ªé…’æ¯ï¼Œè¡¨æƒ…ä¹è§‚å’Œå…´å¥‹ã€‚å‘¨å›´æ˜¯é›¨æ°´çš„å…‰æ³ªï¼Œç»™æ•´ä¸ªåœºæ™¯éƒ½å¸¦æ¥äº†ä¸€å®šçš„æ¼«çº·æ°›ã€‚å¥¹èº«ä½“ä¸­é—´æ˜¯æœ‰å‡ é¢—èŠ±è•¾ï¼Œå¯èƒ½æ˜¯å› ä¸ºå¥¹åœ¨æ˜¥å­£æˆ–è€…ä¸€ä¸ªç‰¹æ®Šçš„æ—¥å­æ‹æ‘„è¿™å¼ ç…§ç‰‡ã€‚ç¯å¢ƒä¸­è¿˜æœ‰ä¸€äº›å•†åº—å’Œå»ºç­‘ç‰©ï¼Œè¡¨æ˜è¿™ä¸ªåœ°æ–¹å¯èƒ½æ˜¯ä¸€ä¸ªç¹åçš„åŸå¸‚åŒºåŸŸã€‚ \n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "from ollama import generate\n",
    "\n",
    "# Pass in the path to the image\n",
    "path = basePath + '/project/github/ComfyUI/output/ComfyUI_00098_.png'\n",
    "img = base64.b64encode(Path(path).read_bytes()).decode()\n",
    "\n",
    "\n",
    "\n",
    "for response in generate('llava', 'ä½¿ç”¨ä¸­æ–‡è§£é‡Šä¸€ä¸‹è¿™ä¸ªå›¾ç‰‡:', images=[img], stream=True):\n",
    "  print(response['response'], end='', flush=True)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## structured-outputs-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"summary\": \"A woman is captured in mid-stride on a rainy night, with her arms outstretched as if embracing the moment. She's wearing a white dress and heels, which contrast with the dark, wet street around her.\",\n",
      "  \"objects\": [\n",
      "    {\n",
      "      \"name\": \"Woman\",\n",
      "      \"confidence\": 0.95,\n",
      "      \"attributes\": \"She is in motion, smiling, and appears to be enjoying herself despite the rain.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Street\",\n",
      "      \"confidence\": 0.85,\n",
      "      \"attributes\": \"The street is wet from recent rainfall, with puddles visible on the asphalt.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Rain\",\n",
      "      \"confidence\": 0.75,\n",
      "      \"attributes\": \"It's raining, creating a reflective sheen on the street and sidewalk.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Dress\",\n",
      "      \"confidence\": 0.85,\n",
      "      \"attributes\": \"The woman is wearing a white dress that stands out against the darker tones of the scene.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Heels\",\n",
      "      \"confidence\": 0.75,\n",
      "      \"attributes\": \"She's wearing high heels, which add an element of style to her outfit.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Street Lights\",\n",
      "      \"confidence\": 0.65,\n",
      "      \"attributes\": \"There are street lights in the background that provide a warm glow despite the rain.\"\n",
      "    }\n",
      "  ],\n",
      "  \"scene\": \"The scene is set on a city street at night during a rain shower.\",\n",
      "  \"colors\": [\n",
      "    \"White\",\n",
      "    \"Black\",\n",
      "    \"Gray\",\n",
      "    \"Red\",\n",
      "    \"Yellow\"\n",
      "  ],\n",
      "  \"time_of_day\": \"Night\",\n",
      "  \"setting\": \"Unknown\",\n",
      "  \"text_content\": \"There is no visible text in the image.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from ollama import chat\n",
    "\n",
    "\n",
    "# Define the schema for image objects\n",
    "class Object(BaseModel):\n",
    "  name: str\n",
    "  confidence: float\n",
    "  attributes: str\n",
    "\n",
    "\n",
    "class ImageDescription(BaseModel):\n",
    "  summary: str\n",
    "  objects: list[Object]\n",
    "  scene: str\n",
    "  colors: list[str]\n",
    "  time_of_day: Literal['Morning', 'Afternoon', 'Evening', 'Night']\n",
    "  setting: Literal['Indoor', 'Outdoor', 'Unknown']\n",
    "  text_content: str | None = None\n",
    "\n",
    "\n",
    "# Get path from user input\n",
    "path = basePath + '/project/github/ComfyUI/output/ComfyUI_00098_.png'\n",
    "path = Path(path)\n",
    "\n",
    "# Verify the file exists\n",
    "if not path.exists():\n",
    "  raise FileNotFoundError(f'Image not found at: {path}')\n",
    "\n",
    "model = 'llava'\n",
    "\n",
    "# Set up chat as usual\n",
    "response = chat(\n",
    "  model=model,\n",
    "  format=ImageDescription.model_json_schema(),  # Pass in the schema for the response\n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': 'Analyze this image and return a detailed JSON description including objects, scene, colors and any text detected. If you cannot determine certain details, leave those fields empty.',\n",
    "      'images': [path],\n",
    "    },\n",
    "  ],\n",
    "  options={'temperature': 0},  # Set temperature to 0 for more deterministic output\n",
    ")\n",
    "\n",
    "\n",
    "# Convert received content to the schema\n",
    "image_analysis = ImageDescription.model_validate_json(response.message.content)\n",
    "\n",
    "# json æ ¼å¼åŒ–è¾“å‡º, åˆ†éš”ç¬¦ 2ä¸ªç©ºæ ¼\n",
    "json_output = image_analysis.model_dump_json(indent=2)\n",
    "print(json_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama ç›¸å…³å‘½ä»¤\n",
    "\n",
    "\n",
    "## ollama --help\n",
    "\n",
    "```bash\n",
    "ollama --help\n",
    "\n",
    "```\n",
    "\n",
    "```text\n",
    "Large language model runner\n",
    "\n",
    "Usage:\n",
    "  ollama [flags]\n",
    "  ollama [command]\n",
    "\n",
    "Available Commands:\n",
    "  serve       Start ollama\n",
    "  create      Create a model from a Modelfile\n",
    "  show        Show information for a model\n",
    "  run         Run a model\n",
    "  stop        Stop a running model\n",
    "  pull        Pull a model from a registry\n",
    "  push        Push a model to a registry\n",
    "  list        List models\n",
    "  ps          List running models\n",
    "  cp          Copy a model\n",
    "  rm          Remove a model\n",
    "  help        Help about any command\n",
    "\n",
    "Flags:\n",
    "  -h, --help      help for ollama\n",
    "  -v, --version   Show version information\n",
    "\n",
    "Use \"ollama [command] --help\" for more information about a command.\n",
    "```\n",
    "\n",
    "\n",
    "* serveï¼šå¯åŠ¨ ollama æœåŠ¡ã€‚\n",
    "* createï¼šæ ¹æ®ä¸€ä¸ª Modelfile åˆ›å»ºä¸€ä¸ªæ¨¡å‹ã€‚\n",
    "* showï¼šæ˜¾ç¤ºæŸä¸ªæ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ã€‚\n",
    "* runï¼šè¿è¡Œä¸€ä¸ªæ¨¡å‹ã€‚\n",
    "* stopï¼šåœæ­¢ä¸€ä¸ªæ­£åœ¨è¿è¡Œçš„æ¨¡å‹ã€‚\n",
    "* pullï¼šä»ä¸€ä¸ªæ¨¡å‹ä»“åº“ï¼ˆregistryï¼‰æ‹‰å–ä¸€ä¸ªæ¨¡å‹ã€‚\n",
    "* pushï¼šå°†ä¸€ä¸ªæ¨¡å‹æ¨é€åˆ°ä¸€ä¸ªæ¨¡å‹ä»“åº“ã€‚\n",
    "* listï¼šåˆ—å‡ºæ‰€æœ‰æ¨¡å‹ã€‚\n",
    "* psï¼šåˆ—å‡ºæ‰€æœ‰æ­£åœ¨è¿è¡Œçš„æ¨¡å‹ã€‚\n",
    "* cpï¼šå¤åˆ¶ä¸€ä¸ªæ¨¡å‹ã€‚\n",
    "* rmï¼šåˆ é™¤ä¸€ä¸ªæ¨¡å‹ã€‚\n",
    "* helpï¼šè·å–å…³äºä»»ä½•å‘½ä»¤çš„å¸®åŠ©ä¿¡æ¯\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/import.md\n",
    "https://github.com/ollama/ollama/blob/main/docs/modelfile.md\n",
    "\n",
    "Create a file named Modelfile, with a FROM instruction with the local filepath to the model you want to import.\n",
    "\n",
    "```Modelfile\n",
    "FROM ./vicuna-33b.Q4_0.gguf\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "# Create the model in Ollama\n",
    "ollama create example -f Modelfile  \n",
    "\n",
    "# Run the model\n",
    "ollama run example\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize a prompt\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/modelfile.md\n",
    "\n",
    "Models from the Ollama library can be customized with a prompt.\n",
    "\n",
    "```Modelfile\n",
    "FROM llama3.2\n",
    "\n",
    "# set the temperature to 1 [higher is more creative, lower is more coherent]\n",
    "PARAMETER temperature 1\n",
    "\n",
    "# set the system message\n",
    "SYSTEM \"\"\"\n",
    "You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "\n",
    "create and run the model:\n",
    "\n",
    "```bash\n",
    "ollama create mario -f ./Modelfile\n",
    "ollama run mario\n",
    "# >>> hi\n",
    "# Hello! It's your friend Mario.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REST API\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "\n",
    "## Generate a response\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/generate -d '{\n",
    "  \"model\": \"llama3.2\",\n",
    "  \"prompt\":\"Why is the sky blue?\"\n",
    "}'\n",
    "\n",
    "# {\n",
    "#   \"model\": \"llama3.2\",\n",
    "#   \"created_at\": \"2023-08-04T08:52:19.385406455-07:00\",\n",
    "#   \"response\": \"The\",\n",
    "#   \"done\": false\n",
    "# }\n",
    "```\n",
    "\n",
    "\n",
    "## Chat with a model\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/chat -d '{\n",
    "  \"model\": \"llama3.2\",\n",
    "  \"messages\": [\n",
    "    { \"role\": \"user\", \"content\": \"why is the sky blue?\" }\n",
    "  ]\n",
    "}'\n",
    "\n",
    "# {\n",
    "#   \"model\": \"llama3.2\",\n",
    "#   \"created_at\": \"2023-08-04T08:52:19.385406455-07:00\",\n",
    "#   \"message\": {\n",
    "#     \"role\": \"assistant\",\n",
    "#     \"content\": \"The\",\n",
    "#     \"images\": null\n",
    "#   },\n",
    "#   \"done\": false\n",
    "# }\n",
    "```\n",
    "\n",
    "\n",
    "## Create a Model\n",
    "\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#create-a-model\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/create -d '{\n",
    "  \"model\": \"mario\",\n",
    "  \"from\": \"llama3.2\",\n",
    "  \"system\": \"You are Mario from Super Mario Bros.\"\n",
    "}'\n",
    "```\n",
    "\n",
    "\n",
    "## List Local Models\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#list-local-models\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/tags\n",
    "```\n",
    "\n",
    "## Show Model Information\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#show-model-information\n",
    "\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/show -d '{\n",
    "  \"model\": \"llama3.2\"\n",
    "}'\n",
    "```\n",
    "\n",
    "## List Running Models\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#list-running-models\n",
    "\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/ps\n",
    "```\n",
    "\n",
    "## Generate Embedding\n",
    "\n",
    "https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embedding\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434/api/embeddings -d '{\n",
    "  \"model\": \"all-minilm\",\n",
    "  \"prompt\": \"Here is an article about llamas...\"\n",
    "}'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export model gguf\n",
    "\n",
    "\n",
    "PyTorch æ ¼å¼ï¼ˆ.bin æˆ– .ptï¼‰ ï¼šåŸå§‹æ¨¡å‹æƒé‡æ–‡ä»¶ï¼Œé€šå¸¸éœ€è¦è½¬æ¢ä¸º GGUF æˆ– GGML æ ¼å¼åæ‰èƒ½ä½¿ç”¨ã€‚  \n",
    "Safetensors æ ¼å¼ ï¼šä¸€ç§å®‰å…¨ä¸”é«˜æ•ˆçš„æƒé‡å­˜å‚¨æ ¼å¼ï¼Œå¸¸ç”¨äº Hugging Face æ¨¡å‹ã€‚  \n",
    "ä¸è¿‡ï¼Œè¿™äº›æ ¼å¼é€šå¸¸ä¸ä¼šç›´æ¥ä½œä¸º Ollama çš„é»˜è®¤æ¨¡å‹æ ¼å¼ï¼Œè€Œæ˜¯éœ€è¦ç»è¿‡è½¬æ¢ã€‚  \n",
    "\n",
    "\n",
    "Ollama ä¼šå°†ä¸‹è½½çš„æ¨¡å‹å­˜å‚¨åœ¨æœ¬åœ°ç›®å½•ä¸­ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹æ–‡ä»¶é€šå¸¸ä½äºä»¥ä¸‹è·¯å¾„ï¼š\n",
    "\n",
    "Linux/macOS : ~/.ollama/models/\n",
    "\n",
    "```bash\n",
    "ollama show deepseek-r1:1.5b --modelfile | head -n 10\n",
    "\n",
    "# Modelfile generated by \"ollama show\"\n",
    "# To build a new Modelfile based on this, replace FROM with:\n",
    "# FROM deepseek-r1:1.5b\n",
    "# FROM ~/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc\n",
    "\n",
    "cd ~/.ollama/models/\n",
    "cat ~/.ollama/models/blobs/sha256-aabd4debf0c8f08881923f2c25fc0fdeed24435271c2b3e92c4af36704040dbc > ollama-export-deepseek-r1-1.5B.gguf\n",
    "```\n",
    "\n",
    "## guuf to PyTorch æ ¼å¼\n",
    "\n",
    "\n",
    "```bash\n",
    "python3 convert-pth-to-ggml.py <path_to_model> <output_path>\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
