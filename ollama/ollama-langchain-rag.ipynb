{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 LangChain 构建本地 RAG 应用\n",
    "\n",
    "\n",
    "https://datawhalechina.github.io/handy-ollama/#/C7/3.%20%E4%BD%BF%E7%94%A8%20LangChain%20%E6%90%AD%E5%BB%BA%E6%9C%AC%E5%9C%B0%20RAG%20%E5%BA%94%E7%94%A8\n",
    "\n",
    "\n",
    "## 环境设置\n",
    "\n",
    "https://github.com/ollama/ollama\n",
    "\n",
    "\n",
    "```bash\n",
    "docker run -d -v /Users/tiankonguse-m3/.ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n",
    "\n",
    "ollama pull llama3.1:8b\n",
    "ollama pull nomic-embed-text\n",
    "\n",
    "conda activate llm-study\n",
    "pip install langchain langchain_community\n",
    "pip install langchain_chroma\n",
    "pip install langchain_ollama\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文档加载\n",
    "\n",
    "现在让我们加载并分割一个示例文档。\n",
    "\n",
    "我们将以 Lilian Weng 的关于 Agent 的 博客 为例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "local_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=local_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们得到了一个本地的向量数据库! 来简单测试一下相似度检索:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id='b93dddaf-bef3-4b0c-bde1-2d7f5f7c42a6', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来实例化大语言模型 llama3.2 并测试模型推理是否正常："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"llama3.2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The scene is set in a dark, crowded nightclub. The crowd is hype, and the judges' table is filled with notable figures from politics and entertainment. In the blue corner, we have Stephen Colbert, aka \"The Late Show\" host, ready to take on his rival from HBO's \"Last Week Tonight\". In the red corner, John Oliver, aka \"The Daily Show\" host, is confident in his lyrical prowess. The battle begins!**\n",
      "\n",
      "**Stephen Colbert:**\n",
      "Yo, John, I heard you've been talking smack\n",
      "About my show, and my wit, right back at that\n",
      "But let me tell you, buddy, I'm the king of the game\n",
      "I make the politicians shiver with shame\n",
      "\n",
      "My words are sharp, like a sword in the night\n",
      "Cutting down lies, and making everything right\n",
      "You may have HBO, but I've got the throne\n",
      "And when it comes to satire, I'm the one to call home\n",
      "\n",
      "**John Oliver:**\n",
      "Hold up, Stephen, let me set the record straight\n",
      "You think you're funny? You're just a late-night mate\n",
      "I tackle tough topics, and make the powerful quake\n",
      "My show's not just about laughs, it's a serious take\n",
      "\n",
      "You may have your Late Show crew, but I've got the might\n",
      "Of \"Last Week Tonight\", shining with all its light\n",
      "We dive into the issues that need some attention\n",
      "And leave you in the dust, like a dirty condition\n",
      "\n",
      "**Stephen Colbert:**\n",
      "Attention? Ha! You think you're the only one?\n",
      "Who's been tackling politics, and having fun?\n",
      "I've got the likes of James Corden, and Seth Meyers too\n",
      "We're not just about jokes, we're comedy gold for you\n",
      "\n",
      "And as for \"Last Week Tonight\", it may be your claim to fame\n",
      "But I'm the one who's been on TV for years, with a legacy to maintain\n",
      "I've got the Late Show crew, and we're not going down\n",
      "You can't touch this, John Oliver, in this battle crown\n",
      "\n",
      "**John Oliver:**\n",
      "Fun? Ha! You think comedy's just about having fun?\n",
      "It's about being incisive, and getting the job done\n",
      "My show may not be as flashy, but it gets to the heart\n",
      "And when it comes to holding power accountable, we're never apart\n",
      "\n",
      "You may have your show, but I've got the numbers on my side\n",
      "We're the ones who matter, in this information age ride\n",
      "I'm not just a comedian, I'm a journalist too\n",
      "My show's the one that matters, and I'll see it through\n"
     ]
    }
   ],
   "source": [
    "response_message = model.invoke(\n",
    "    \"Simulate a rap battle between Stephen Colbert and John Oliver\"\n",
    ")\n",
    "\n",
    "print(response_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建 Chain 表达形式\n",
    "\n",
    "\n",
    "我们可以通过传入检索到的文档和简单的 prompt 来构建一个 summarization chain 。\n",
    "\n",
    "它使用提供的输入键值格式化提示模板，并将格式化后的字符串传递给指定的模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main themes in these retrieved docs are:\\n\\n1. **Task Decomposition**: Breaking down complex tasks into smaller, manageable subgoals to enable efficient handling.\\n2. **Planning and Execution**: A crucial component of an autonomous agent system, where expert models execute on specific tasks and log results.\\n3. **Autonomy and Self-Awareness**: The ability of the agent to plan, reflect, and refine its actions to improve the quality of final results.\\n\\nAdditionally, the docs mention three methods for task decomposition:\\n\\n1. Using simple prompting from a Large Language Model (LLM) with instructions like \"Steps for XYZ.\"\\n2. Using task-specific instructions, such as \"Write a story outline.\"\\n3. Utilizing human inputs and feedback.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "\n",
    "# 将传入的文档转换成字符串的形式\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "chain = {\"docs\": format_docs} | prompt | model | StrOutputParser()\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单QA\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are three approaches to task decomposition: (1) using a Large Language Model (LLM) with simple prompting, (2) using task-specific instructions, and (3) incorporating human inputs. These approaches enable agents to break down complex tasks into smaller subgoals, enhancing efficient handling of complicated tasks. Task-specific instructions provide more precise guidance than simple prompts or human input.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "RAG_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Answer the following question:\n",
    "\n",
    "{question}\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(context=lambda input: format_docs(input[\"context\"]))\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "docs = vectorstore.similarity_search(question)\n",
    "\n",
    "# Run\n",
    "chain.invoke({\"context\": docs, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 带有检索的QA\n",
    "\n",
    "最后，我们带有语义检索功能的 QA 应用（本地 RAG 应用），可以根据用户问题自动从向量数据库中检索语义上最相近的文档片段：\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are three approaches to task decomposition: (1) using simple prompting with a Large Language Model (LLM), (2) utilizing task-specific instructions, and (3) leveraging human inputs. These methods allow agents to break down complex tasks into manageable subgoals, enabling efficient handling of complicated tasks. Additionally, reflection and refinement can be employed to improve the quality of results.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "\n",
    "qa_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "恭喜，至此，你已经完整的实现了一个基于 Langchain 框架和本地模型构建的 RAG 应用。你可以在教程的基础上替换本地模型来尝试不同模型的效果和能力，或进一步进行扩展，丰富应用的能力和表现力，或者添加更多实用有趣的功能。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
