{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchain\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homePath:  /Users/tiankonguse\n",
      "basePath:  /Users/tiankonguse\n"
     ]
    }
   ],
   "source": [
    "# python 获取系统变量 HOME 变量\n",
    "import os\n",
    "homePath = os.environ['HOME']\n",
    "print(\"homePath: \",homePath)\n",
    "\n",
    "# 修改成自己的 HOME 路径\n",
    "basePath=homePath\n",
    "\n",
    "print(\"basePath: \",basePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/integrations/chat/\n",
    "%pip install -qU langchain\n",
    "%pip install -qU langchain-groq\n",
    "%pip install -qU langchain-openai\n",
    "%pip install -qU openai\n",
    "%pip install -qU langchain-ollama\n",
    "%pip install -qU langchain-community\n",
    "%pip install langchain-huggingface\n",
    "%pip install --upgrade --quiet  dashscope #ChatTongyi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "#   os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")\n",
    "  os.environ[\"GROQ_API_KEY\"] = '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'\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# https://groq.com/#\n",
    "# https://console.groq.com/playground?model=llama3-8b-8192\n",
    "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\n",
    "\n",
    "response = model.invoke(\"Who are you?\")\n",
    "\n",
    "print(response.content)\n",
    "# I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm not a human, but a computer program designed to simulate conversation and answer questions to the best of my knowledge. I'm here to help you with any questions or topics you'd like to discuss, so feel free to ask me anything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture 架构\n",
    "\n",
    "* langchain-core: Base abstractions for chat models and other components.\n",
    "* Integration packages (e.g. langchain-openai, langchain-anthropic, etc.): Important integrations have been split into lightweight packages that are * co-maintained by the LangChain team and the integration developers.\n",
    "* langchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\n",
    "* langchain-community: Third-party integrations that are community maintained.\n",
    "* langgraph: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See LangGraph documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorials\n",
    "\n",
    "https://python.langchain.com/docs/tutorials/\n",
    "\n",
    "## Get started\n",
    "\n",
    "https://python.langchain.com/docs/tutorials/#get-started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Chat models and prompts\n",
    "\n",
    "https://python.langchain.com/docs/tutorials/llm_chain/\n",
    "\n",
    "Build a simple LLM application with prompt templates and chat models.\n",
    "\n",
    "\n",
    "This application will translate text from English into another language. \n",
    "\n",
    "After reading this tutorial, you'll have a high level overview of:\n",
    "\n",
    "- Using language models\n",
    "- Using prompt templates\n",
    "- Debugging and tracing your application using LangSmith\n",
    "\n",
    "#### Installation\n",
    "\n",
    "```bash\n",
    "pip install langchain\n",
    "```\n",
    "\n",
    "#### LangSmith\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Language Models\n",
    "\n",
    "https://python.langchain.com/docs/integrations/chat/\n",
    "\n",
    "安装大模型的 python SDK\n",
    "\n",
    "https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install -U langchain-openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      5\u001b[39m   os.environ[\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m] = getpass.getpass(\u001b[33m\"\u001b[39m\u001b[33mEnter API key for OpenAI: \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_models\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m init_chat_model\n\u001b[32m      9\u001b[39m model = init_chat_model(\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m, model_provider=\u001b[33m\"\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "# OPENAI_API_KEY=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\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你好！', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_06737a9306', 'finish_reason': 'stop', 'logprobs': None}, id='run-45f284ea-eab4-4975-9520-1fc6cb923b86-0', usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the following from English into Chinese\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)\n",
    "\n",
    "# LangChain also supports chat model inputs via strings or OpenAI format. The following are equivalent:\n",
    "# model.invoke(\"Hello\")\n",
    "# model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])\n",
    "# model.invoke([HumanMessage(\"Hello\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Streaming\n",
    "\n",
    "https://python.langchain.com/docs/how_to/chat_streaming/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "你好\n",
      "！\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token in model.stream(messages):\n",
    "    print(token.content, end=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Templates\n",
    "\n",
    "This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. \n",
    "\n",
    "Common transformations include adding a system message or formatting a template with the user input.\n",
    "\n",
    "\n",
    "\n",
    "[Prompt templates](https://python.langchain.com/docs/concepts/prompt_templates/) are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following from English into {language}\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({\"language\": \"Chinese\", \"text\": \"hi!\"})\n",
    "\n",
    "prompt.to_messages()\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
