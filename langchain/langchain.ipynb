{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchain\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU \"langchain[groq]\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm not a human, but a computer program designed to simulate conversation and answer questions to the best of my knowledge. I'm here to help you with any questions or topics you'd like to discuss, so feel free to ask me anything!\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "#   os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")\n",
    "  os.environ[\"GROQ_API_KEY\"] = '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'\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# https://groq.com/#\n",
    "# https://console.groq.com/playground?model=llama3-8b-8192\n",
    "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\n",
    "\n",
    "response = model.invoke(\"who are you?\")\n",
    "\n",
    "print(response.content)\n",
    "# I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm not a human, but a computer program designed to simulate conversation and answer questions to the best of my knowledge. I'm here to help you with any questions or topics you'd like to discuss, so feel free to ask me anything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture 架构\n",
    "\n",
    "* langchain-core: Base abstractions for chat models and other components.\n",
    "* Integration packages (e.g. langchain-openai, langchain-anthropic, etc.): Important integrations have been split into lightweight packages that are * co-maintained by the LangChain team and the integration developers.\n",
    "* langchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\n",
    "* langchain-community: Third-party integrations that are community maintained.\n",
    "* langgraph: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See LangGraph documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorials\n",
    "\n",
    "https://python.langchain.com/docs/tutorials/\n",
    "\n",
    "## Get started\n",
    "\n",
    "https://python.langchain.com/docs/tutorials/#get-started\n",
    "\n",
    "### Chat models and prompts\n",
    "\n",
    "https://python.langchain.com/docs/tutorials/llm_chain/\n",
    "\n",
    "Build a simple LLM application with prompt templates and chat models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
